[{"title":"python-函数","date":"2018-11-07T11:33:42.000Z","path":"2018/11/07/Python-函数/","text":"函数参数12345def info(name, age, country = &apos;CN&apos;, *args, **kwargs): # 必填参数、默认值参数、可变参数、关键字参数 name = &apos;rearib&apos; # 局部变量 print(name)func(&apos;rearib&apos;, 12) # 位置测试func(age=22, name=&apos;rearib&apos;) # 关键字参数 name是形式参数 ‘rearib’是传入的实参 country是默认参数, 传入了默认值 位置参数传参，需要按顺序 关键字参数传参，可以不按顺序 局部变量作用域在函数内部，不影响外部 *args会把多传入的参数变成一个元祖形式 **kwargs会把多传入的参数变成一个dict形式 递归函数一个函数在内部调用自身, 就叫递归函数 必须有一个明确的结束条件； 每次进入更深一层递归时，问题规模相比上次递归都应有所减少； 递归效率不高，递归层次过多会导致栈溢出（在计算机中，函数调用是通过栈（stack）这种数据结构实现的，每当进入一个函数调用，栈就会加一层栈帧，每当函数返回，栈就会减一层栈帧。由于栈的大小不是无限的，所以，递归调用的次数过多，会导致栈溢出 高阶函数满足下列条件之一就可称函数为高阶函数 某一函数当做参数传入另一个函数中 函数的返回值包含一个或多个函数 map()函数map()是Python内置的高阶函数，它接收一个函数f和一个list，并通过把函数f依次作用在list的每个元素上，得到一个新的list并返回123456def func(x): return x * xa= map(func,range(1,10))print(list(a))[1, 4, 9, 16, 25, 36, 49, 64, 81] filter()函数filter()函数接收一个函数f和一个list，这个函数f的作用是对每个元素进行判断，返回True或False，filter()根据判断结果自动过滤掉不符合条件的元素，返回由符合条件元素组成的新list123456def is_odd(x): return x % 2 == 1a = filter(is_odd,[1,2,3,4,5,6,7,8])print(list(a))[1, 3, 5, 7] 匿名函数lambdalambda函数是一种快速定义单行的最小函数 lambda 传入的参数:参数处理1234567def fun(x,y): return x + ya = lambda x,y:x + yfun(1,2)a(1,2) 函数嵌套和作用域在一个函数中定义了另外一个函数, 参数依据就近原则, 从里往外找 闭包 学习Javascript闭包 装饰器 高阶+嵌套1234567891011121314def outer(func): def inner(name): print(&apos;start&apos;) func(name) print(&apos;stop&apos;) return inner@outer # hello = outer(hello)(name)def hello(name): print(&apos;welcome&apos;, name)hello(&apos;Tom&apos;) 多个装饰器123456789101112131415161718192021222324252627def outer1(func1): print(&apos;outer1&apos;) def inner1(*args, **kwargs): print(&apos;start&apos;) r = func1(*args, **kwargs) print(&apos;end&apos;) return r return inner1def outer2(func2): print(&apos;outer2&apos;) def inner2(*args, **kwargs): print(&apos;开始&apos;) r = func2(*args, **kwargs) print(&apos;结束&apos;) return r return inner2@outer1 @outer2 def f(): print(&apos;f 函数&apos;)f() 执行@outer1和@outer2时会重下往上执行, 先执行outer2并返回inner2, 所以f = outer2(f) = inner2 然后执行outer1, 所以f = outer1(inner2) = inner1, 然后执行f()相对于执行innrt1(inner2), 所以先打印’start’之后跳到inner2执行打印’开始’、’f 函数’、’结束’ 最后返回到inner1中打印’end’ 多个装饰器的应用1234@is_login@is_admindef add_student(name): print(&quot;添加学生信息.....&quot;) 首先我们调用的is_admin函数，在运行完is_admin外层函数时发现还有一个装饰器函数，然后把参数传向is_login函数，把is_login函数外层函数运行结束后，系统发现并没有第三个装饰器，那么is_login里的warpper函数就是最底层函数，那么它就依次执行完is_login里的所有函数，再原路返回去寻找上一层函数直到函数运行结束； 装饰器再传递一层参数12345678910111213141516171819202122232425262728293031323334import timedef timer(parameter): def outer_wrapper(func): def wrapper(*args, **kwargs): if parameter == &apos;task1&apos;: start = time.time() func(*args, **kwargs) stop = time.time() print(&quot;the task1 run time is :&quot;, stop - start) elif parameter == &apos;task2&apos;: start = time.time() func(*args, **kwargs) stop = time.time() print(&quot;the task2 run time is :&quot;, stop - start) return wrapper return outer_wrapper@timer(parameter=&apos;task1&apos;)def task1(): time.sleep(2) print(&quot;in the task1&quot;)@timer(parameter=&apos;task2&apos;)def task2(): time.sleep(2) print(&quot;in the task2&quot;)task1()task2() Python内置函数 Python3的内置函数 bytes() 将一个字符串转换成字节类型 1234&gt;&gt;&gt; a = &apos;王&apos;&gt;&gt;&gt; s = bytes(a, encoding=&apos;utf-8&apos;)&gt;&gt;&gt; sb&apos;\\xe7\\x8e\\x8b&apos;","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"Django-对cookie、session、token、jwt的理解","date":"2018-10-05T14:46:43.000Z","path":"2018/10/05/Django-对cookie、session、token、jwt的理解/","text":"&emsp;&emsp;HTTP协议是一个无状态的协议，也就是说每个请求都是一个独立的请求，请求与请求之间并无关系。但在实际的应用场景，这种方式并不能满足我们的需求。举个大家都喜欢用的例子，把商品加入购物车，单独考虑这个请求，服务端并不知道这个商品是谁的，应该加入谁的购物车？因此这个请求的上下文环境实际上应该包含用户的相关信息，在每次用户发出请求时把这一小部分额外信息，也做为请求的一部分，这样服务端就可以根据上下文中的信息，针对具体的用户进行操作。所以这几种技术的出现都是对HTTP协议的一个补充。使得我们可以用HTTP协议+状态管理构建一个的面向用户的WEB应用。 Cookie和Session&emsp;&emsp;Session与Cookie最核心区别在于额外信息由谁来维护： 利用cookie来实现会话管理时，用户的相关信息或者其他我们想要保持在每个请求中的信息，都是放在cookie中，而cookie是由客户端来保存，每当客户端发出新请求时，就会稍带上cookie，服务端会根据其中的信息进行操作。 利用session来进行会话管理时，客户端实际上只存了一个由服务端发送的session_id，而由这个session_id，可以在服务端还原出所需要的所有状态信息，所以Session是由服务端来维护的。cookie是怎么工作的？&emsp;&emsp;当网页要发http请求时，浏览器会先检查是否有相应的cookie，有则自动添加在request header中的cookie字段中。这些是浏览器自动帮我们做的，而且每一次http请求浏览器都会自动帮我们做。 Cookie的设置 12rep = HttpResponse(...) 或 rep ＝ render(request, ...)rep.set_cookie(&apos;username&apos;,&apos;admin&apos;) 示例代码 123456789101112131415161718192021222324252627def login(request): if request.method == &apos;GET&apos;: return render(request,&apos;login.html&apos;) if request.method == &apos;POST&apos;: u = request.POST.get(&apos;username&apos;) p = request.POST.get(&apos;pwd&apos;) dic = user_info.get(u) if not dic: return render(request,&apos;login.html&apos;) if dic[&apos;pwd&apos;] == p: result = redirect(&apos;/index/&apos;) result.set_cookie(&apos;username&apos;,u) return result else: return render(request,&apos;login.html&apos;)def auth(func): def inner(request,*args,**kwargs): v = request.COOKIES.get(&apos;username&apos;) if not v: return redirect(&apos;/login/&apos;) return func(request,*args,**kwargs) return inner@authdef index(request): v = request.COOKIES.get(&apos;username&apos;) return render(request,&apos;index.html&apos;,&#123;&apos;current_user&apos;:v&#125;) 第一次打开浏览器，进入index，会自动跳转到login，因为没有cookie：现输入用户名密码登录，通过上面的代码可得会在返回的时候set_cookie，以key-value的形式：再将浏览器关闭，重新打开index，发现会跳转到login，是因为默认设置为浏览器关闭就会删除cookie。现修改result.set_cookie(&#39;username&#39;,&#39;u&#39; ,max_age=60)，重新登录：过期时间是一分钟之后，在此期间就算关闭浏览器再次打开cookie还是有效。 cookie使用key-value的形式保存数据在客户端 创建一个cookie的生命周期默认为-1，即max_age=-1，表示当浏览器关闭时cookie消失，如果设置max_age=0，表示通知浏览器删除cookie。 cookie 参数 key：键 value=’’：值 max_age=None：超时时间（秒） expires=None：超时时间(IE requires expires, so set it if hasn’t been already.) 单位日期 secure=False：当secure值为true时，cookie在HTTP中是无效，在HTTPS中才有效。 httponly=False：浏览器不允许脚本操作 document.cookie去更改cookie。一般情况下都应该设置这个为true，这样可以避免被xss攻击拿到cookie。 path：表示cookie所在的目录，默认为/，就是根目录。假如在同一个服务器上有目录如下：/test/,/test/cd/,/test/dd/，现设一个cookie1的path为/test/，cookie2的path为/test/cd/，那么test下的所有页面都可以访问到cookie1，而/test/和/test/dd/的子页面不能访问cookie2。这是因为cookie只能让其path路径下的页面访问。 domain：表示的是cookie所在的域，默认为请求的地址，如网址为www.test.com/test/test.aspx，那么domain默认为www.test.com。而跨域访问，如域A为t1.test.com，域B为t2.test.com，那么在域A生产一个令域A和域B都能访问的cookie就要将该cookie的domain设置为.test.com；如果要在域A生产一个令域A不能访问而域B能访问的cookie就要将该cookie的domain设置为t2.test.com。 利用cookie登录 首先使用谷歌浏览器打开博客园登录，然后进入调试状态，查看cookie：打开火狐浏览器，进入博客园，现在是未登录状态：使用cookie Editor编辑cookie:再重新刷新页面： cookie缺点 cookie的长度和数量有限制 cookie安全性不高，不适合存放敏感数据 每次请求需要带上cookie信息，增加流量消耗 Cookie具有不可跨域名性。Google也只能操作Google的Cookie，而不能操作Baidu的Cookie。 session是怎么工作的?&emsp;&emsp;session的运作通过一个session_id来进行。session_id通常是存放在客户端的cookie中，当你下次访问时，cookie会带有这个字符串，然后浏览器就知道你是上次访问过的某某某，然后从服务器的存储中取出上次记录在你身上的数据。由于字符串是随机产生的，而且位数足够多，所以也不担心有人能够伪造。 Django的session &emsp;&emsp;Django中默认支持Session，其内部提供了5种类型的Session供开发者使用： 数据库（默认） 缓存 文件 缓存+数据库 加密cookie 编辑MIDDLEWARE设置，并确保它包含django.contrib.sessions.middleware.SessionMiddleware，在中间件的process_request中获取request的cookie，其中SESSION_COOKIE_NAME = &#39;sessionid&#39;：123def process_request(self, request): session_key = request.COOKIES.get(settings.SESSION_COOKIE_NAME) request.session = self.SessionStore(session_key) 在process_response中设置cookie：1234567response.set_cookie(settings.SESSION_COOKIE_NAME, request.session.session_key, max_age=max_age, expires=expires, domain=settings.SESSION_COOKIE_DOMAIN, path=settings.SESSION_COOKIE_PATH, secure=settings.SESSION_COOKIE_SECURE or None, httponly=settings.SESSION_COOKIE_HTTPONLY or None, ) session的缺点 &emsp;&emsp;session在一定的时间里，需要存放在服务端，因此当拥有大量用户时，也会大幅度降低服务端的性能。&emsp;&emsp;当有多台机器时，如何共享session也会是一个问题，也就是说，用户第一个访问的时候是服务器A，而第二个请求被转发给了服务器B，那服务器B如何得知其状态。 Token验证&emsp;&emsp;API应该被设计成无状态的。这意味着没有登陆，注销的方法，也没有sessions，API的设计者同样也不能依赖Cookie，因为不能保证这些request是由浏览器所发出的。自然，我们需要一个新的机制。所以提出了Token验证。 流程 客户端使用用户名跟密码请求登录 服务端收到请求，去验证用户名与密码 验证成功后，服务端会签发一个 Token，再把这个 Token 发送给客户端 客户端收到 Token 以后可以把它存储起来，比如放在 Cookie 里或者 Local Storage 里 客户端每次向服务端请求资源的时候需要带着服务端签发的 Token 服务端收到请求，然后去验证客户端请求里面带着的 Token，如果验证成功，就向客户端返回请求的数据 如果用户退出登录，token会在客户端销毁，这一步与服务器无关 token与session及cookie的关系 &emsp;&emsp;借助session识别用户是否登录的方式，客户端cookie中用于追踪session的那个值（sessionId）可被称为token，这个token通常是用户名+密码认证成功后所得到的。由此可见session与cookie跟token产生联系，是因为token的概念恰好有一个用session+cookie实现的场景。 &emsp;&emsp;token就是token，只是在特定场景中与session、cookie产生了联系，因此我们完全可以选择不借助session与cookie而使用token，比如在HTTP请求头中使用x-token头部传输token的值，又或者是在URL中携带token，这两者在如今都是较常见的做法。 session的传递一般都是通过cookie来传递的，或者url重写的方式；而token在服务器是可以不需要存储用户的信息的，而token的传递方式也不限于cookie传递，当然，token也是可以保存起来的。 浏览器第一次访问服务器，根据传过来的唯一标识userId，服务端会通过一些算法，如常用的HMAC-SHA256算法，然后加一个密钥，生成一个token，然后通过BASE64编码一下之后将这个token发送给客户端；客户端将token保存起来，下次请求时，带着token，服务器收到请求后，然后会用相同的算法和密钥去验证token，如果通过，执行业务操作，不通过，返回不通过信息。 session一般翻译为会话，而token更多的时候是翻译为令牌，session是空间换时间，而token是时间换空间。 drf的token &emsp;&emsp;关于drf如何使用token参考文章Django REST framework+Vue 打造生鲜超市（六） 1.访问url(r&#39;^api-token-auth/&#39;, views.obtain_auth_token),传过来的是username和password,会生成一个token返回回去 2.访问其他网页在header中携带token就可以获取到userAuthorization:Token 89bd7209bf9d5c4656a4f24bb120a1e3b9587ccd drf的token缺点 保存在数据库中，如果是一个分布式的系统，就非常麻烦 token永久有效，没有过期时间。 JWT&emsp;&emsp;本质上来说 JWT 也是 token，它是 JSON 结构的 token，由三部分组成：1) header 2) payload 3) signature 。&emsp;&emsp;JWT 的目的不是为了隐藏或者保密数据，而是为了确保数据确实来自被授权的人创建的（不被篡改） JWT和Token区别:JWT经过加密不易修改,而且可以设置过期时间,比如7天,7天后携带该JWT过来会要求重新登录 参考文章: 理解Cookie和Session机制 Session,Token相关区别 Token ，Cookie和Session的区别 前后端分离之JWT用户认证 Node 实作jwt 验证API","tags":[{"name":"Django","slug":"Django","permalink":"http://yoursite.com/tags/Django/"}]},{"title":"Django-中间件MiddleWare","date":"2018-10-04T14:46:43.000Z","path":"2018/10/04/Django-中间件MiddleWare/","text":"什么是中间件&emsp;&emsp;中间件是一个用来处理Django的请求和响应的框架级别的钩子。它是一个轻量、低级别的插件系统，用于在全局范围内改变Django的输入和输出。每个中间件组件都负责做一些特定的功能。但是由于其影响的是全局，所以需要谨慎使用，使用不当会影响性能。 中间件配置&emsp;&emsp;在django项目的settings模块中，有一个 MIDDLEWARE 变量，其中每一个元素就是一个中间件。 中间件方法和执行顺序&emsp;&emsp;对于中间件，可以定义下面五种方法：12345process_request(self,request)process_view(self, request, callback, callback_args, callback_kwargs)process_template_response(self,request,response)process_exception(self, request, exception)process_response(self, request, response) &emsp;&emsp;以上方法的返回值可以是None或一个HttpResponse对象，如果是None，则继续按照django定义的规则向后继续执行，如果是HttpResponse对象，则直接将该对象返回给用户。 process_request &emsp;&emsp;process_request有一个参数，就是request，这个request和视图函数中的request是一样的。&emsp;&emsp;它的返回值可以是None也可以是HttpResponse对象。返回值是None的话，按正常流程继续走，交给下一个中间件处理，如果是HttpResponse对象，Django将不执行视图函数，而将相应对象返回给浏览器。 中间件的process_request方法是在执行视图函数之前执行的。 当配置多个中间件时，会按照MIDDLEWARE中的注册顺序，也就是列表的索引值，从前到后依次执行的。 不同中间件之间传递的request都是同一个对象 process_view process_view(self, request, view_func, view_args, view_kwargs) request是HttpRequest对象。 view_func是Django即将使用的视图函数。 （它是实际的函数对象，而不是函数的名称作为字符串。） view_args是将传递给视图的位置参数的列表. view_kwargs是将传递给视图的关键字参数的字典。 view_args和view_kwargs都不包含第一个视图参数（request）。 Django会在调用视图函数之前调用process_view方法。 &emsp;&emsp;它应该返回None或一个HttpResponse对象。 如果返回None，Django将继续处理这个请求，执行任何其他中间件的process_view方法，然后在执行相应的视图。 如果它返回一个HttpResponse对象，Django不会调用适当的视图函数。 它将执行中间件的process_response方法并将应用到该HttpResponse并返回结果。 process_exception process_exception(self, request, exception) request是HttpRequest对象。 exception是视图函数异常产生的Exception对象。 如果视图函数中无异常，process_exception方法不执行。 &emsp;&emsp;这个方法只有在视图函数中出现异常了才执行，它返回的值可以是一个None也可以是一个HttpResponse对象。如果是HttpResponse对象，Django将调用模板和中间件中的process_response方法，并返回给浏览器，否则将默认处理异常。如果返回一个None，则交给下一个中间件的process_exception方法来处理异常。它的执行顺序也是按照中间件注册顺序的倒序执行。 process_template_response（用的比较少） process_template_response(self, request, response) request是HttpRequest对象。 response是TemplateResponse对象（由视图函数或者中间件产生）。 &emsp;&emsp;process_template_response是在视图函数执行完成后立即执行，但是它有一个前提条件，那就是视图函数返回的对象有一个render()方法（或者表明该对象是一个TemplateResponse对象或等价方法） process_response &emsp;&emsp;它有两个参数，一个是request，一个是response，request就是上述例子中一样的对象，response是视图函数返回的HttpResponse对象。该方法的返回值也必须是HttpResponse对象。 多个中间件中的process_response方法是按照MIDDLEWARE中的注册顺序倒序执行的，也就是说第一个中间件的process_request方法首先执行，而它的process_response方法最后执行，最后一个中间件的process_request方法最后一个执行，它的process_response方法是最先执行。 中间件的执行流程图&emsp;&emsp;请求到达中间件之后，先按照正序执行每个注册中间件的process_request方法，process_request方法返回的值是None，就依次执行，如果返回的值是HttpResponse对象，不再执行后面的process_request方法，而是执行当前对应中间件的process_response方法，将HttpResponse对象返回给浏览器。也就是说：如果MIDDLEWARE中注册了6个中间件，执行过程中，第3个中间件返回了一个HttpResponse对象，那么第4,5,6中间件的process_request和process_response方法都不执行，顺序执行3,2,1中间件的process_response方法。&emsp;&emsp;process_request方法都执行完后，匹配路由，找到要执行的视图函数，先不执行视图函数，先执行中间件中的process_view方法，process_view方法返回None，继续按顺序执行，所有process_view方法执行完后执行视图函数。加入中间件3 的process_view方法返回了HttpResponse对象，则4,5,6的process_view以及视图函数都不执行，直接从最后一个中间件，也就是中间件6的process_response方法开始倒序执行。process_template_response和process_exception两个方法的触发是有条件的，执行顺序也是倒序。总结所有的执行流程如下：&emsp;&emsp;在执行完RequestMiddlewares后先到urls.py然后再执行ViewMiddlewares中间件 源码分析&emsp;&emsp;在前面我们探究过web框架的本质，在实例化一个WSGIHander对象的时候，在__init__()中通过self.load_middleware()会加载setting中设置的中间件。&emsp;&emsp;从settings配置文件读取设置的middleware，然后初始化WSGIHandler类中的各个middleware的相关变量，这些变量主要包括self._request_middleware，self._view_middleware，self._template_response_middleware，self._response_middleware，self._exception_middleware，均为存放中间件方法的列表。 中间件的应用&emsp;&emsp;由于中间件工作在视图函数执行前、执行后（像不像所有视图函数的装饰器！）适合所有的请求/一部分请求做批量处理。 做IP限制： 放在 中间件类的列表中，阻止某些IP访问； URL访问过滤 如果用户访问的是login视图（放过） 如果访问其他视图（需要检测是不是有session已经有了放行，没有返回login），这样就省得在多个视图函数上写装饰器了！ 缓存 客户端请求来了，中间件去缓存看看有没有数据，有直接返回给用户，没有再去逻辑层执行视图函数 参考文章: django源码分析之请求响应流程 Django 中间件原理及源码分析 Django 中间件 官方文档","tags":[{"name":"Django","slug":"Django","permalink":"http://yoursite.com/tags/Django/"}]},{"title":"Django-model模型知识点总结","date":"2018-10-02T14:46:43.000Z","path":"2018/10/02/Django-model模型知识点总结/","text":"model梳理与难点剖析Python操作数据库&emsp;&emsp;用Python语法来写，然后使用一个中间工具将Python代码翻译成原生的SQL语句，这个中间工具就是所谓的ORM（对象关系映射）！ORM将一个Python的对象映射为数据库中的一张关系表。它将SQL封装起来，程序员不再需要关心数据库的具体操作，只需要专注于自己本身代码和业务逻辑的实现。于是，整体的实现过程就是：Python代码，通过ORM转换成SQL语句，再通过pymysql去实际操作数据库。&emsp;&emsp;最典型的ORM就是SQLAlchemy了，但是Django自带ORM系统，不需要额外安装别的ORM。 字段选项和字段类型 阅读文章 官方文档 刘江的个人博客 null和Black的区别: &emsp;&emsp;如果null为True，Django将在数据库中将空值存储为NUL，blank=True，表单验证时将允许输入空值，null 纯粹是数据库范畴的概念，而blank 是数据验证范畴的。 自增字段 1id = models.AutoField(primary_key=True) auto_now_add和auto_add auto_now:每当对象被保存时将字段设为当前日期，常用于保存最后修改时间。 auto_now_add：每当对象被创建时，设为当前日期，常用于保存创建日期(注意，它是不可修改的)。 db_column字段列名称 &emsp;&emsp;数据库中用db_column来表示该字段的列名称。 如果未指定，那么Django将会使用字段名作为列名。1id = models.AutoField(max_length=11,db_column=&apos;UID&apos;,primary_key=True) unique_for_date &emsp;&emsp;如果你有一个title 字段设置unique_for_date=”pub_date”，那么Django 将不允许两个记录具有相同的title 和pub_date。unique_for_month类似unique_for_date，只是要求字段对于月份是唯一的。 upload_to &emsp;&emsp;用于设置上传地址的目录和文件名:123456class MyModel(models.Model): # 文件被传至`MEDIA_ROOT/uploads`目录，MEDIA_ROOT由你在settings文件中设置 upload = models.FileField(upload_to=&apos;uploads/&apos;) # 或者 # 被传到`MEDIA_ROOT/uploads/2015/01/30`目录，增加了一个时间划分 upload = models.FileField(upload_to=&apos;uploads/%Y/%m/%d/&apos;) &emsp;&emsp;也可以接收回调函数:123456def user_directory_path(instance, filename): #文件上传到MEDIA_ROOT/user_&lt;id&gt;/&lt;filename&gt;目录中 return &apos;user_&#123;0&#125;/&#123;1&#125;&apos;.format(instance.user.id, filename)class MyModel(models.Model): upload = models.FileField(upload_to=user_directory_path) 评论系统的外键 &emsp;&emsp;什么时候需要自己引用自己的外键呢？典型的例子就是评论系统！一条评论可以被很多人继续评论:1parent_comment = models.ForeignKey(&apos;self&apos;, on_delete=models.CASCADE) choice &emsp;&emsp;在choices中，每个元组中的第一个元素，是存储在数据库中的值；第二个元素是使人容易理解的描述。给定一个模型实例，可以使用get_FOO_display()方法来访问选项字段的显示值。123456YEAR_IN_SCHOOL_CHOICES = ( (&apos;FR&apos;, &apos;Freshman&apos;), (&apos;SO&apos;, &apos;Sophomore&apos;), (&apos;JR&apos;, &apos;Junior&apos;), (&apos;SR&apos;, &apos;Senior&apos;),) related_name和related_query_name的使用：官方文档 &emsp;&emsp;假如有下面两个模型:12345678class Person(models.Model); name = models.CharField(verbose_name=&apos;作者姓名&apos;, max_length=10) age = models.IntegerField(verbose_name=&apos;作者年龄&apos;)class Book(models.Model): person = models.ForeignKey(Person, related_name=&apos;person_book&apos;) title = models.CharField(verbose_name=&apos;书籍名称&apos;, max_length=10) pubtime = models.DateField(verbose_name=&apos;出版时间&apos;) &emsp;&emsp;如果我们要查询一个作者出版了哪些书籍的话,先查询到作者的信息,person = Person.objects.fiter(你的条件),返回一个person对象,然后反向查询:book = person.book_set.all(),但是如果定义了related_name,就可以使用person.person_books.all()。 关联关系字段 on_delete设置 &emsp;&emsp;ForeignKey.on_delete：当删除由ForeignKey引用的对象时，Django将模拟由on_delete参数指定的SQL约束的行为。 through和through_fields的作用 &emsp;&emsp;ManyToMany时会自动创建第三张表用于管理多对多关系，如果想在第三张表添加额外字段，可以使用through：参考链接&emsp;&emsp;当中间模型具有多个外键指向多对多关联关系模型中的任何一个（或两个），你必须指定through_fields：官方文档 1234567891011121314151617181920212223from django.db import modelsclass Person(models.Model): name = models.CharField(max_length=50)class Group(models.Model): name = models.CharField(max_length=128) members = models.ManyToManyField( Person, through=&apos;Membership&apos;, through_fields=(&apos;group&apos;, &apos;person&apos;), )class Membership(models.Model): group = models.ForeignKey(Group, on_delete=models.CASCADE) person = models.ForeignKey(Person, on_delete=models.CASCADE) //邀请者 inviter = models.ForeignKey( Person, on_delete=models.CASCADE, related_name=&quot;membership_invites&quot;, ) invite_reason = models.CharField(max_length=64) &emsp;&emsp;Membership有两个 foreign keys指向 Person (person and inviter), 这样会导致关系不清晰，Django不知道使用哪一个外键。 在这种情况下，你必须使用through_fields 明确指定Django 应该使用哪些外键，就像上面例子一样。&emsp;&emsp;通俗的说，就是through_fields参数指定从中间表模型Membership中选择哪两个字段，作为关系连接字段。 模型的Meta选项 官方文档 abstract = True：就表示模型是抽象基类，抽象模型本身不实际生成数据库表。 verbose_name = “pizza”：对象的一个易于理解的名称 unique_together = (“driver”, “restaurant”)：用来设置的不重复的字段组合,当unique_together的约束被违反时，模型校验期间会抛出ValidationError异常。 ordering = [‘-order_date’]：对象默认的顺序 model的Manager示例 &emsp;&emsp;每个非抽象的Model 类必须给自己添加一个Manager实例。 Django 确保在你的模型类中至少有一个默认的Manager。 如果你没有添加自己的管理器，Django将添加一个属性objects。&emsp;&emsp;自定义管理器 验证对象 官方文档 &emsp;&emsp;验证一个模型涉及三个步骤： 验证模型的字段 —— Model.clean_fields() 验证模型的完整性 —— Model.clean() 验证模型的唯一性 —— Model.validate_unique() &emsp;&emsp;当你调用模型的full_clean() 方法时，这三个方法都将执行。 信号 发出预先保存的信号。 发送pre_save信号，允许监听该信号的任何函数执行某些操作。 预处理数据。 调用每个字段的pre_save()方法来执行所需的任何自动数据修改。 例如，日期/时间字段覆盖pre_save()来实现auto_now_add和auto_now。 准备数据库的数据。 要求每个字段的get_db_prep_save()方法将其当前值提供给可写入数据库的数据类型。大多数字段不需要数据准备。 简单的数据类型，例如整数和字符串，是可以直接写入的Python 对象。 但是，复杂的数据类型通常需要一些改动。例如，DateField 字段使用Python 的 datetime 对象来保存数据。 数据库保存的不是datetime 对象，所以该字段的值必须转换成ISO兼容的日期字符串才能插入到数据库中。 将数据插入数据库。 预处理的准备数据组成一个用于插入数据库的SQL语句。 发出一个保存后的信号。 发送post_save信号，允许监听该信号的任何函数执行某些操作。 现有字段更新 &emsp;&emsp;有时候你需要在一个字段上执行简单的算法操作，例如增加或者减少当前值。 实现这点的简单方法是像下面这样：123&gt;&gt;&gt; product = Product.objects.get(name=&apos;Venezuelan Beaver Cheese&apos;)&gt;&gt;&gt; product.number_sold += 1&gt;&gt;&gt; product.save() &emsp;&emsp;如果从数据库中读取的旧的number_sold 值为10，那么写回到数据库中的值将为11。&emsp;&emsp;通过将更新基于原始字段的值而不是显式赋予一个新值，这个过程可以避免竞态条件而且更快。 Django提供F表达式用于这种类型的相对更新。 利用F表达式，前面的示例可以表示成：1234&gt;&gt;&gt; from django.db.models import F&gt;&gt;&gt; product = Product.objects.get(name=&apos;Venezuelan Beaver Cheese&apos;)&gt;&gt;&gt; product.number_sold = F(&apos;number_sold&apos;) + 1&gt;&gt;&gt; product.save() 指定要保存的字段 &emsp;&emsp;如果传递给save() 的update_fields 关键字参数一个字段名称列表，那么将只有该列表中的字段会被更新。12product.name = &apos;Name changed again&apos;product.save(update_fields=[&apos;name&apos;]) 实例方法 __str__() 方法返回模型的一个友好的、人类可读的形式。 get_absolute_url() 方法告诉Django 如何计算对象的标准URL。 get_FOO_display():对于每个具有choices 的字段，每个对象将具有一个get_FOO_display() 方法，其中FOO 为该字段的名称。 这个方法返回该字段对“人类可读”的值。 QuerySet QuerySet何时求值官方文档 返回新的QuerySet的方法官方文档 不返回QuerySet的方法官方文档 聚合函数官方文档 exists()方法 &emsp;&emsp;如果你需要知道是否存在至少一条记录（而不需要真实的对象），使用 exists() 将更加高效。 aggregate和annotate &emsp;&emsp;博客：django中聚合aggregate和annotate GROUP BY的使用方法&emsp;&emsp;官方文档：Aggregate()表达式&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;annotate表达式 order_by方法 &emsp;&emsp; 升序是隐含的。 要随机订购，请使用”?”。 values方法 1Blog.objects.filter(name__startswith=&apos;Beatles&apos;).values() &emsp;&emsp;返回一个返回字典的QuerySet，而不是使用模型实例作为一个迭代。每个字典表示一个对象，键对应于模型对象的属性名称。 value_list()方法 &emsp;&emsp;与values() 类似，只是在迭代时返回的是元组而不是字典。 Django的Q对象和F对象 &emsp;&emsp;博客：Python3之Django Web框架F对象，Q对象&emsp;&emsp;官方文档：F()表达式 抽象模型 刘江的个人博客 多表继承(默认隐含一个OneToOneField来链接子类与父类)123456789from django.db import modelsclass Place(models.Model): name = models.CharField(max_length=50) address = models.CharField(max_length=80)class Restaurant(Place): serves_hot_dogs = models.BooleanField(default=False) serves_pizza = models.BooleanField(default=False) Restaurant可以看做是Place的扩展,所以调用只能:1234&gt;&gt;&gt; p = Place.objects.get(id=12)# 如果p也是一个Restaurant对象，那么下面的调用可以获得该Restaurant对象。&gt;&gt;&gt; p.restaurant&lt;Restaurant: ...&gt; 在多表继承的情况下，由于父类和子类都在数据库内有物理存在的表，父类的Meta类会对子类造成不确定的影响，因此，Django在这种情况下关闭了子类继承父类的Meta功能。这一点和抽象基类的继承方式有所不同。 代理模型&emsp;&emsp;只需要将Meta中proxy的值设为True。&emsp;&emsp;可以在代理模型中改变默认的排序方式和默认的manager管理器等等，而不会对原始模型产生影响。12345678910111213from django.db import modelsclass Person(models.Model): first_name = models.CharField(max_length=30) last_name = models.CharField(max_length=30)class MyPerson(Person): class Meta: proxy = True def do_something(self): # ... pass 字段查询 &emsp;&emsp;字段查询其实就是filter()、exclude()和get()等方法的关键字参数。 其基本格式是：field__lookuptype=value，注意其中是双下划线。","tags":[{"name":"Django","slug":"Django","permalink":"http://yoursite.com/tags/Django/"}]},{"title":"Django-Web框架本质探究","date":"2018-10-01T14:46:43.000Z","path":"2018/10/01/Django-Web框架本质探究/","text":"Web框架本质&emsp;&emsp;对于所有的Web应用，本质上其实就是一个socket服务端，用户的浏览器其实就是一个socket客户端。对于真实开发中的python web程序来说，一般会分为两部分：服务器程序和应用程序。服务器程序负责对socket服务器进行封装，并在请求到来时，对请求的各种数据进行整理。应用程序则负责具体的逻辑处理。&emsp;&emsp;为了方便应用程序的开发,就会出现许多的Web框架,而所有的web框架都需要与服务器程序配合才能为用户服务，所以web框架和web服务器之间需要标准化。 一般Web框架架构&emsp;&emsp;大多数基于Python的web框架，如Django、tornado、flask、webpy都是在这个范围内进行增删裁剪的。例如Tornado用的是自己的异步非阻塞“WSGI”网关接口，Flask则只提供了最精简和基本的框架，Django则是直接使用了现成的WSGI，并实现了大部分功能，提供了大量的应用工具。 MTV设计模式 WSGI-uwsgi-uWSGI区别&emsp;&emsp;WSGI：全称是Web Server Gateway Interface，WSGI不是服务器、python模块、框架、API或者任何软件，只是一种网关接口，它是一个Web服务器（如nginx，uWSGI等服务器）与web应用（如用Django框架写的程序）通信的一种规范。&emsp;&emsp;uwsgi：一种传输协议，常用于在uWSGI服务器与其他网络服务器的数据通信。&emsp;&emsp;uWSGI：是一个web服务器，实现了WSGI协议、uwsgi协议、http协议等。工作流程: 首先客户端请求服务资源。 nginx作为直接对外的服务接口,接收到客户端发送过来的http请求,会解包、分析。 如果是静态文件请求就根据nginx配置的静态文件目录，返回请求的资源。 如果是动态的请求,nginx就通过配置文件,将请求传递给uWSGI。 uWSGI将接收到的包进行处理，转发给wsgi。 wsgi根据请求调用django工程的某个文件或函数，处理完后django将返回值交给wsgi。 wsgi将返回值进行打包，转发给uWSGI。 uWSGI接收后转发给nginx,nginx最终将返回值返回给客户端(如浏览器)。 &emsp;&emsp;第一级的nginx并不是必须的，uwsgi完全可以完成整个的和浏览器交互的流程，但是要考虑到某些情况： 安全问题：程序不能直接被浏览器访问到，而是通过nginx。nginx只开放某个接口，这样运维人员在nginx上加上安全性的限制，可以达到保护程序的作用。 负载均衡问题：一个uwsgi很可能不够用，有了nginx做代理，一个nginx可以代理多台uwsgi完成uwsgi的负载均衡。 静态文件问题：用django或是uwsgi这种东西来负责静态文件的处理是很浪费的行为，而且他们本身对文件的处理也不如nginx好，所以整个静态文件的处理都直接由nginx完成，静态文件的访问完全不去经过uwsgi以及其后面的东西。 通过对Django的启动研究socket的启动&emsp;&emsp;使用runserver是启动Django自带的WSGI服务器，常用于开发调试，分析Django的源码: 执行python manage.py runserver 127.0.0.1:8000后，会在\\Django1.11.6\\Lib\\site-packages\\django\\core\\management\\__init__.py中执行self.fetch_command(subcommand).run_from_argv(self.argv)。 调用fetch_command返回的runserver模块下的Command对象，其中Command对象是&lt;django.contrib.staticfiles.management.commands.runserver.Command object&gt;，继续调用Command对象的run_from_argv方法，然后执行self.handle(*args, **options)调用Command下的handle。 从Command.handle–&gt;Command.run–&gt;Command.inner_run，并在inner_run中执行： 123handler = self.get_handler(*args, **options)run(self.addr, int(self.port), handler, ipv6=self.use_ipv6, threading=threading, server_cls=self.server_cls) 在get_handler中会在通过wsgi.py返回WSGIHandler()，继承于class WSGIHandler(base.BaseHandler)，然后会执行run: 而run就是启动一个线程一直监听，其中wsgi_handler是WSGIHandler()，server_cls=WSGIServer是继承于class WSGIServer(simple_server.WSGIServer, object)，而simple_server来自于from wsgiref import simple_server，而wsgiref就是Python内置的一个WSGI服务器。 1234567891011121314151617def run(addr, port, wsgi_handler, ipv6=False, threading=False, server_cls=WSGIServer): server_address = (addr, port) if threading: httpd_cls = type(str(&apos;WSGIServer&apos;), (socketserver.ThreadingMixIn, server_cls), &#123;&#125;) else: httpd_cls = server_cls httpd = httpd_cls(server_address, WSGIRequestHandler, ipv6=ipv6) if threading: # ThreadingMixIn.daemon_threads indicates how threads will behave on an # abrupt shutdown; like quitting the server by the user or restarting # by the auto-reloader. True means the server will not wait for thread # termination before it quits. This will make auto-reloader faster # and will prevent the need to kill the server manually if a thread # isn&apos;t terminating correctly. httpd.daemon_threads = True httpd.set_app(wsgi_handler) httpd.serve_forever() wsgiref是用纯Python编写的WSGI服务器的参考实现。所谓“参考实现”是指该实现完全符合WSGI标准，但是不考虑任何运行效率，仅供开发和测试使用。 参考文章: 由django.setup()引发得对Django启动过程解读 Django 源码学习之搭建环境和django启动流程(一)","tags":[{"name":"Django","slug":"Django","permalink":"http://yoursite.com/tags/Django/"}]},{"title":"python-队列与生产者消费者模式","date":"2018-09-21T11:47:08.000Z","path":"2018/09/21/Python-队列与生产者消费者模式/","text":"&emsp;&emsp;并发编程中使用生产者和消费者模式能够解决绝大多数并发问题。该模式通过平衡生产线程和消费线程的工作能力来提高程序的整体处理数据的速度。1、为什么要使用生产者和消费者模式&emsp;&emsp;在线程世界里，生产者就是生产数据的线程，消费者就是消费数据的线程。在多线程开发当中，如果生产者处理速度很快，而消费者处理速度很慢，那么生产者就必须等待消费者处理完，才能继续生产数据。同样的道理，如果消费者的处理能力大于生产者，那么消费者就必须等待生产者。为了解决这个问题于是引入了生产者和消费者模式。2、什么是生产者消费者模式&emsp;&emsp;生产者消费者模式是通过一个容器来解决生产者和消费者的强耦合问题。生产者和消费者彼此之间不直接通讯，而通过阻塞队列来进行通讯，所以生产者生产完数据之后不用等待消费者处理，直接扔给阻塞队列，消费者不找生产者要数据，而是直接从阻塞队列里取，阻塞队列就相当于一个缓冲区，平衡了生产者和消费者的处理能力。 队列&emsp;&emsp;队列（Queue）:在多个线程之间安全的交换数据信息，队列在多线程编程中特别有用队列的好处： 提高双方的效率，你只需要把数据放到队列中，中间去干别的事情。 完成了程序的解耦性，两者关系依赖性没有不大。 队列的类型： lass queue.Queue(maxsize=0) 先进先出，后进后出12345678import queueq = queue.Queue() # 生成先入先出队列实例q.put(1) # 先放进1，再放入2q.put(2)print(q.get()) #输出1 class queue.LifoQueue(maxsize=0) 先进后出，后进先出12345678import queueq = queue.LifoQueue() # 生成先入先出队列实例q.put(1) # 先放进1，再放入2q.put(2)print(q.get()) #输出2 class queue.PriorityQueue(maxsize=0)优先级来取数据。存放数据的格式 : Queue.put((priority_number,data))，priority_number越小，优先级越高，data代表存入的值12345678910111213import queueq = queue.PriorityQueue()q.put((1, &quot;d1&quot;))q.put((-1, &quot;d2&quot;))q.put((6, &quot;d3&quot;))print(q.get())print(q.get())print(q.get()) #输出(-1, &apos;d2&apos;)(1, &apos;d1&apos;)(6, &apos;d3&apos;) maxsize代表这个队列最大能够put的长度，如果xsize &lt;= 0，则队列大小为无限大。 队列的内置方法 exception queue.Empty队列中的数据为空时，就会抛出这个异常。 12345678910import queueq = queue.Queue()q.get(block=False) #获取不到的时候#输出Traceback (most recent call last): File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt; File &quot;D:\\Python\\Python35\\lib\\queue.py&quot;, line 161, in get raise Emptyqueue.Empty exception queue.Full当队列中满了以后，再放数据的话，就会抛出此异常。 12345678910111213import queueq = queue.Queue(maxsize=1) q.put(1)q.put(1,block=False)#输出Traceback (most recent call last): File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt; File &quot;D:\\Python\\Python35\\lib\\queue.py&quot;, line 184, in put_nowait return self.put(item, block=False) File &quot;D:\\Python\\Python35\\lib\\queue.py&quot;, line 130, in put raise Fullqueue.Full Queue.qsize()查看队列的大小。 12345678import queueq = queue.Queue() q.put(20)q.put(21)print(q.qsize()) #查看队列的大小#输出2 Queue.empty()队列如果为空返回True，不为空返回False。 12345678910import queueq = queue.Queue() q.put(1)print(q.empty()) #查看队列是否为空q.get()print(q.empty()) #查看队列是否为空#输出FalseTrue Queue.full()队列如果满了，返回True，没有满返回False。 12345678910import queueq = queue.Queue(maxsize=1) q.put(1)print(q.full()) #查看队列是否满q.get()print(q.full()) #查看队列是否满#输出TrueFalse Queue.put(item,block=True,timeout=None)把数据插入队列中。block参数默认为true，timeout默认值是None。如果blcok为false的话，那么在put时候超过设定的maxsize的值，就会报full 异常。如果timeout设置值得话，说明put值得个数超过maxsize值，那么会在timeout几秒之后抛出full异常。 1234567891011import queueq = queue.Queue(maxsize=1) #是定队列的大小为1q.put(1)q.put(1,block=False) #block不会阻塞，会full异常#输出Traceback (most recent call last): File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt; File &quot;D:\\Python\\Python35\\lib\\queue.py&quot;, line 130, in put raise Fullqueue.Full 1234567891011import queueq = queue.Queue(maxsize=1) #是定队列的大小为1q.put(1)q.put(1,timeout=1) #超过1秒，则会报full异常#输出Traceback (most recent call last): File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt; File &quot;D:\\Python\\Python35\\lib\\queue.py&quot;, line 130, in put raise Fullqueue.Full Queue.put_nowait(item)等同于Queue.put(item,block=False)或者是Queue.put(item,False)。 123456789101112import queueq = queue.Queue(maxsize=1)q.put(1)q.put_nowait(1) #等同于q.put(1,block=False)#输出Traceback (most recent call last): File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt; File &quot;D:\\Python\\Python35\\lib\\queue.py&quot;, line 184, in put_nowait return self.put(item, block=False) File &quot;D:\\Python\\Python35\\lib\\queue.py&quot;, line 130, in put raise Fullqueue.Full Queue.get(block=True,timeout=None)移除并返回队列中的序列。参数block=true并且timeout=None。如果block=false的话，那么队列为空的情况下，就直接Empty异常。如果timeout有实际的值，这个时候队列为空，执行get的时候，则时隔多长时间则报出Empty的异常。 1234567891011import queueq = queue.Queue()q.put(1)q.get()q.get(block=False) #获取不到值，直接抛Empty异常#输出Traceback (most recent call last): File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt; File &quot;D:\\Python\\Python35\\lib\\queue.py&quot;, line 161, in get raise Emptyqueue.Empty 1234567891011import queueq = queue.Queue()q.put(1)q.get()q.get(timeout=1) #设置超时时间，抛出Empty异常#输出Traceback (most recent call last): File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt; File &quot;D:\\Python\\Python35\\lib\\queue.py&quot;, line 172, in get raise Emptyqueue.Empty Queue.get_nowait(item)等同于Queue.get(block=False)或者Queue.get(False)。 12345678910111213import queueq = queue.Queue()q.put(1)q.get()q.get_nowait() #等同于q.get(block=False)#输出Traceback (most recent call last): File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt; File &quot;D:\\Python\\Python35\\lib\\queue.py&quot;, line 192, in get_nowait return self.get(block=False) File &quot;D:\\Python\\Python35\\lib\\queue.py&quot;, line 161, in get raise Emptyqueue.Empty Queue.task_done()get()用于获取任务，task_done()则是用来告诉队列之前获取的任务已经处理完成 Queue.join()block(阻塞)直到queue（队列）被消费完毕,如果生产者生产10个包子，那么要等消费者把这个10个包子全部消费完毕，生产者才能继续往下执行。 task_done和jion的理解 生成者消费者模型例子生产者生产完毕，消费者再消费例子：1234567891011121314151617181920212223import threadingimport queue def producer(): for i in range(10): q.put(&quot;骨头 %s&quot; % i) print(&quot;开始等待所有的骨头被取走...&quot;) q.join() # 等待这个骨头队列被消费完毕 print(&quot;所有的骨头被取完了...&quot;)def consumer(n): while q.qsize() &gt; 0: print(&quot;%s 取到&quot; % n, q.get()) q.task_done() # 每去到一个骨头，便告知队列这个任务执行完了 q = queue.Queue() p = threading.Thread(target=producer,)p.start() c1 = consumer(&quot;QQ&quot;) 边生产边消费的模型例子1234567891011121314151617181920212223242526272829import time,randomimport queue,threadingq = queue.Queue() def producer(name): count = 0 while count &lt; 10: time.sleep(random.randrange(3)) q.put(count) # 在队列里放包子 print&apos;Producer %s has produced %s baozi..&apos; (% (name, count)) count += 1 def consumer(name): count = 0 while count &lt; 10: time.sleep(random.randrange(4)) if not q.empty(): # 如果还有包子 data = q.get() # 就继续获取保证 print(data) print(&apos;Consumer %s has eat %s baozi...&apos; % (name, data)) else: print(&quot;-----no baozi anymore----&quot;) count += 1 p1 = threading.Thread(target=producer, args=(&apos;A&apos;,))c1 = threading.Thread(target=consumer, args=(&apos;B&apos;,))p1.start()c1.start() 流程图: 生产者生产，消费者消费。 消费者每消费一次，都要去执行以下task_done()方法，来告诉消费者已经消费成功，相当于吃完饭，消费者应该给钱了。 消费者每消费一次，则队列中计数器会做减1操作。 当队列中的计数器为0的时候，则生产者不阻塞，继续执行，不为0的时候，则阻塞，直到消费者消费完毕为止。","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"python-RabbitMQ 安装、基本示例、轮询机制","date":"2018-09-21T05:23:47.000Z","path":"2018/09/21/Python-RabbitMQ 安装、基本示例、轮询机制/","text":"&emsp;&emsp;MQ全称为Message Queue, 是一种分布式应用程序的的通信方法，它是消费-生产者模型的一个典型的代表，producer往消息队列中不断写入消息，而另一端consumer则可以读取或者订阅队列中的消息。RabbitMQ是MQ产品的典型代表，是一款基于AMQP协议可复用的企业消息系统。业务上，可以实现服务提供者和消费者之间的数据解耦，提供高可用性的消息传输机制，在实际生产中应用相当广泛。本文意在介绍Rabbitmq的基本原理，以及在python下的各种应用。 python中的queue概念： 线程queue：只是用于多个线程之间，进行数据同步交互的。 进程queue：只是用户父进程与子进程进行交互，或者属于同一父进程下的多个子进程进行交互。","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"python-事件驱动和IO介绍","date":"2018-09-18T03:33:42.000Z","path":"2018/09/18/Python-事件驱动和IO介绍/","text":"&emsp;&emsp;通常，我们写服务器处理模型的程序时，有以下几种模型： 每收到一个请求，创建一个新的进程，来处理该请求； 每收到一个请求，创建一个新的线程，来处理该请求； 每收到一个请求，放入一个事件列表，让主进程通过非阻塞I/O方式来处理请求 &emsp;&emsp;上面的几种方式，各有千秋： 第1中方法，由于创建新的进程的开销比较大，所以，会导致服务器性能比较差,但实现比较简单。 第2种方式，由于要涉及到线程的同步，有可能会面临死锁等问题。 第3种方式，在写应用程序代码时，逻辑比前面两种都复杂。&emsp;&emsp;综合考虑各方面因素，一般普遍认为第（3）种方式是大多数网络服务器采用的方式事件驱动模型&emsp;&emsp;在UI编程中，常常要对鼠标点击进行相应，首先如何获得鼠标点击呢？ 方式一：创建一个线程，该线程一直循环检测是否有鼠标点击，那么这个方式有以下几个缺点： CPU资源浪费，可能鼠标点击的频率非常小，但是扫描线程还是会一直循环检测，这会造成很多的CPU资源浪费；如果扫描鼠标点击的接口是阻塞的呢？ 如果是堵塞的，又会出现下面这样的问题，如果我们不但要扫描鼠标点击，还要扫描键盘是否按下，由于扫描鼠标时被堵塞了，那么可能永远不会去扫描键盘； 如果一个循环需要扫描的设备非常多，这又会引来响应时间的问题；所以，该方式是非常不好的。 方式二：就是事件驱动模型,目前大部分的UI编程都是事件驱动模型，如很多UI平台都会提供onClick()事件，这个事件就代表鼠标按下事件。事件驱动模型大体思路如下： 有一个事件（消息）队列； 鼠标按下时，往这个队列中增加一个点击事件（消息）； 有个循环，不断从队列取出事件，根据不同的事件，调用不同的函数，如onClick()、onKeyDown()等； 事件（消息）一般都各自保存各自的处理函数指针，这样，每个消息都有独立的处理函数； &emsp;&emsp;事件驱动编程是一种编程范式，这里程序的执行流由外部事件来决定。它的特点是包含一个事件循环，当外部事件发生时使用回调机制来触发相应的处理。另外两种常见的编程范式是（单线程）同步以及多线程编程。 &emsp;&emsp;让我们用例子来比较和对比一下单线程、多线程以及事件驱动编程模型。下图展示了随着时间的推移，这三种模式下程序所做的工作。这个程序有3个任务需要完成，每个任务都在等待I/O操作时阻塞自身。阻塞在I/O操作上所花费的时间已经用灰色框标示出来了。 &emsp;&emsp;在单线程同步模型中，任务按照顺序执行。如果某个任务因为I/O而阻塞，其他所有的任务都必须等待，直到它完成之后它们才能依次执行。这种明确的执行顺序和串行化处理的行为是很容易推断得出的。如果任务之间并没有互相依赖的关系，但仍然需要互相等待的话这就使得程序不必要的降低了运行速度。 &emsp;&emsp;在多线程版本中，这3个任务分别在独立的线程中执行。这些线程由操作系统来管理，在多处理器系统上可以并行处理，或者在单处理器系统上交错执行。这使得当某个线程阻塞在某个资源的同时其他线程得以继续执行。与完成类似功能的同步程序相比，这种方式更有效率，但程序员必须写代码来保护共享资源，防止其被多个线程同时访问。多线程程序更加难以推断，因为这类程序不得不通过线程同步机制如锁、可重入函数、线程局部存储或者其他机制来处理线程安全问题，如果实现不当就会导致出现微妙且令人痛不欲生的bug。 &emsp;&emsp;在事件驱动版本的程序中，3个任务交错执行，但仍然在一个单独的线程控制中。当处理I/O或者其他昂贵的操作时，注册一个回调到事件循环中，然后当I/O操作完成时继续执行。回调描述了该如何处理某个事件。事件循环轮询所有的事件，当事件到来时将它们分配给等待处理事件的回调函数。这种方式让程序尽可能的得以执行而不需要用到额外的线程。事件驱动型程序比多线程程序更容易推断出行为，因为程序员不需要关心线程安全问题。 &emsp;&emsp;当我们面对如下的环境时，事件驱动模型通常是一个好的选择： 程序中有许多任务，而且任务之间高度独立（因此它们不需要互相通信，或者等待彼此）,而且在等待事件到来时，某些任务会阻塞。 当应用程序需要在任务间共享可变的数据时，这也是一个不错的选择，因为这里不需要采用同步处理。 网络应用程序通常都有上述这些特点，这使得它们能够很好的契合事件驱动编程模型。 &emsp;&emsp;上面的事件驱动模型中，只要一遇到IO就注册一个事件，然后主程序就可以继续干其它的事情了，只到io处理完毕后，继续恢复之前中断的任务，这本质上是怎么实现的呢？ 阻塞IO,非阻塞IO,同步IO,异步IO介绍&emsp;&emsp;对于一次IO访问（以read举例），数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。所以说，当一个read操作发生时，它会经历两个阶段： 等待数据准备 (Waiting for the data to be ready) 将数据从内核拷贝到进程中 (Copying the data from the kernel to the process) &emsp;&emsp;正式因为这两个阶段，linux系统产生了下面五种网络模式的方案。 阻塞 I/O（blocking IO） 非阻塞 I/O（nonblocking IO） I/O 多路复用（ IO multiplexing） 信号驱动 I/O（ signal driven IO） 异步 I/O（asynchronous IO） 注：由于signal driven IO在实际中并不常用，所以只提及剩下的四种IO Model。 1、概念说明 1.1、用户空间与内核空间 &emsp;&emsp;现在操作系统都是采用虚拟存储器，那么对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方）。操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操心系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。针对linux操作系统而言，将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），供内核使用，称为内核空间，而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF），供各个进程使用，称为用户空间。 1.2、进程切换 &emsp;&emsp;为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的。 &emsp;&emsp;从一个进程的运行转到另一个进程上运行，这个过程中经过下面这些变化： 保存处理机上下文，包括程序计数器和其他寄存器。 更新PCB信息。 把进程的PCB移入相应的队列，如就绪、在某事件阻塞等队列。 选择另一个进程执行，并更新其PCB。 更新内存管理的数据结构。 恢复处理机上下文。 &emsp;&emsp;总而言之就是很耗资源，具体的可以参考这篇文章：进程切换 &emsp;&emsp;注：进程控制块（Processing Control Block），是操作系统核心中一种数据结构，主要表示进程状态。其作用是使一个在多道程序环境下不能独立运行的程序（含数据），成为一个能独立运行的基本单位或与其它进程并发执行的进程。或者说，OS是根据PCB来对并发执行的进程进行控制和管理的。 PCB通常是系统内存占用区中的一个连续存区，它存放着操作系统用于描述进程情况及控制进程运行所需的全部信息 1.3、进程阻塞 &emsp;&emsp;正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态。可见，进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得CPU），才可能将其转为阻塞状态。当进程进入阻塞状态，是不占用CPU资源的。 1.4、文件描述符fd &emsp;&emsp;文件描述符（File descriptor）是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。 &emsp;&emsp;文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。 1.5、缓存I/O &emsp;&emsp;缓存 I/O 又被称作标准 I/O，大多数文件系统的默认 I/O 操作都是缓存 I/O。在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。 &emsp;&emsp;缓存 I/O 的缺点：数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。 五种IO网络模式 阻塞 I/O（blocking IO） &emsp;&emsp;在linux中，默认情况下所有的socket都是blocking，一个典型的读操作流程大概是这样：&emsp;&emsp;当用户进程调用了recvfrom这个系统调用，kernel（内核）就开始了IO的第一个阶段：准备数据（对于网络IO来说，很多时候数据在一开始还没有到达。比如，还没有收到一个完整的UDP包。这个时候kernel就要等待足够的数据到来）。这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程会被阻塞（当然，是进程自己选择的阻塞）。当kernel一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。 所以，blocking IO的特点就是在IO执行的两个阶段都被block了。 非阻塞 I/O（nonblocking IO） &emsp;&emsp;linux下，可以通过设置socket使其变为non-blocking。当对一个non-blocking socket执行读操作时，流程是这个样子： &emsp;&emsp;当用户进程发出read操作时，如果kernel（内核）中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。从用户进程角度讲 ，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。 所以，nonblocking IO的特点是用户进程需要不断的主动询问kernel数据好了没有。 I/O 多路复用（ IO multiplexing） &emsp;&emsp;IO multiplexing就是我们说的select，poll，epoll，有些地方也称这种IO方式为event driven IO。select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。 &emsp;&emsp;当用户进程调用了select，那么整个进程会被block，而同时，kernel会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel（内核）拷贝到用户进程。 &emsp;&emsp;这个图和blocking IO的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个system call (select 和 recvfrom)，而blocking IO只调用了一个system call (recvfrom)。但是，用select的优势在于它可以同时处理多个connection。 &emsp;&emsp;所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。） &emsp;&emsp;在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。 所以，I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select()函数就可以返回。 异步 I/O（asynchronous IO） &emsp;&emsp;Linux下的asynchronous IO其实用得很少。先看一下它的流程：&emsp;&emsp;用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel（内核）会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。 总结： blocking和non-blocking的区别 &emsp;&emsp;调用blocking IO会一直block住对应的进程直到操作完成，而non-blocking IO在kernel（内核）还准备数据的情况下会立刻返回。 synchronous IO和asynchronous IO的区别 &emsp;&emsp;两者的区别就在于synchronous IO做”IO operation”的时候会将process阻塞。按照这个定义，之前所述的blocking IO，non-blocking IO，IO multiplexing都属于synchronous IO。 &emsp;&emsp;有人会说，non-blocking IO并没有被block啊。这里有个非常“狡猾”的地方，定义中所指的”IO operation”是指真实的IO操作，就是例子中的recvfrom这个system call。non-blocking IO在执行recvfrom这个system call的时候，如果kernel的数据没有准备好，这时候不会block进程。但是，当kernel中数据准备好的时候，recvfrom会将数据从kernel拷贝到用户内存中，这个时候进程是被block了，在这段时间内，进程是被block的。 &emsp;&emsp;而asynchronous IO则不一样，当进程发起IO 操作之后，就直接返回再也不理睬了，直到kernel发送一个信号，告诉进程说IO完成。在这整个过程中，进程完全没有被block。 各个IO Model的比较如图所示： IO多路复用（select、poll、epoll）介绍及select、epoll的实现&emsp;&emsp;IO多路复用中包括 select、pool、epoll，这些都属于同步，还不属于异步 1、select&emsp;&emsp;select最早于1983年出现在4.2BSD中，它通过一个select()系统调用来监视多个文件描述符的数组，当select()返回后，该数组中就绪的文件描述符便会被内核修改标志位，使得进程可以获得这些文件描述符从而进行后续的读写操作。 &emsp;&emsp;select目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点，事实上从现在看来，这也是它所剩不多的优点之一。 &emsp;&emsp;select的一个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024，不过可以通过修改宏定义甚至重新编译内核的方式提升这一限制。 &emsp;&emsp;另外，select()所维护的存储大量文件描述符的数据结构，随着文件描述符数量的增大，其复制的开销也线性增长。同时，由于网络响应时间的延迟使得大量TCP连接处于非活跃状态，但调用select()会对所有socket进行一次线性扫描，所以这也浪费了一定的开销。 2、poll&emsp;&emsp;poll在1986年诞生于System V Release 3，它和select在本质上没有多大差别，但是poll没有最大文件描述符数量的限制。 &emsp;&emsp;poll和select同样存在一个缺点就是，包含大量文件描述符的数组被整体复制于用户态和内核的地址空间之间，而不论这些文件描述符是否就绪，它的开销随着文件描述符数量的增加而线性增大。 &emsp;&emsp;另外，select()和poll()将就绪的文件描述符告诉进程后，如果进程没有对其进行IO操作，那么下次调用select()和poll()的时候将再次报告这些文件描述符，所以它们一般不会丢失就绪的消息，这种方式称为水平触发（Level Triggered）。 3、epoll&emsp;&emsp;直到Linux2.6才出现了由内核直接支持的实现方法，那就是epoll，它几乎具备了之前所说的一切优点，被公认为Linux2.6下性能最好的多路I/O就绪通知方法。 &emsp;&emsp;epoll可以同时支持水平触发和边缘触发（Edge Triggered，只告诉进程哪些文件描述符刚刚变为就绪状态，它只说一遍，如果我们没有采取行动，那么它将不会再次告知，这种方式称为边缘触发），理论上边缘触发的性能要更高一些，但是代码实现相当复杂。 &emsp;&emsp;epoll同样只告知那些就绪的文件描述符，而且当我们调用epoll_wait()获得就绪文件描述符时，返回的不是实际的描述符，而是一个代表就绪描述符数量的值，你只需要去epoll指定的一个数组中依次取得相应数量的文件描述符即可，这里也使用了内存映射（mmap）技术，这样便彻底省掉了这些文件描述符在系统调用时复制的开销。 &emsp;&emsp;另一个本质的改进在于epoll采用基于事件的就绪通知方式。在select/poll中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描，而epoll事先通过epoll_ctl()来注册一个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似callback的回调机制，迅速激活这个文件描述符，当进程调用epoll_wait()时便得到通知。 select IO多路复用&emsp;&emsp;Python的select()方法直接调用操作系统的IO接口，它监控sockets,open files, and pipes(所有带fileno()方法的文件句柄)何时变成readable 和writeable, 或者通信错误，select()使得同时监控多个连接变的简单，并且这比写一个长循环来等待和监控多客户端连接要高效，因为select直接通过操作系统提供的C的网络接口进行操作，而不是通过Python的解释器。 &emsp;&emsp;select目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。select的一 个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024，可以通过修改宏定义甚至重新编译内核的方式提升这一限制，但是这样会造成效率的降低 继续看","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"python-协程","date":"2018-09-17T11:36:42.000Z","path":"2018/09/17/Python-协程/","text":"&emsp;&emsp;协程，又称微线程。英文名Coroutine。一句话说明什么是协程：协程是一种用户态的轻量级线程。&emsp;&emsp;协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈。因此：&emsp;&emsp;协程能保留上一次调用时的状态（即所有局部状态的一个特定组合），每次过程重入时，就相当于进入上一次调用的状态，换种说法：进入上一次离开时所处逻辑流的位置。 协程的好处： 无需线程上下文切换的开销 无需原子操作锁定及同步的开销(协程是单线程,串行) “原子操作(atomic operation)是不需要synchronized”，所谓原子操作是指不会被线程调度机制打断的操作；这种操作一旦开始，就一直运行到结束，中间不会有任何 context switch （切换到另一个线程）。原子操作可以是一个步骤，也可以是多个操作步骤，但是其顺序是不可以被打乱，或者切割掉只执行部分。视作整体是原子性的核心。 方便切换控制流，简化编程模型 高并发+高扩展性+低成本：一个CPU支持上万的协程都不是问题。所以很适合用于高并发处理。 协程的缺点： 无法利用多核资源：协程的本质是个单线程,它不能同时将 单个CPU 的多个核用上,协程需要和进程配合才能运行在多CPU上.当然我们日常所编写的绝大部分应用都没有这个必要，除非是cpu密集型应用。 进行阻塞（Blocking）操作（如IO时）会阻塞掉整个程序。 yield实现协程123456789101112131415161718192021222324import timedef consumer(name): print(&quot;---&gt;starting eating baozi...&quot;) while True: new_baozi = yield # yield设置生成器 print(&quot;[&#123;0&#125;] is eating baozi &#123;1&#125;&quot;.format(name, new_baozi)) def producer(): r = con.__next__() # 调用生成器开始执行 r = con2.__next__() n = 0 while n &lt; 5: time.sleep(1) print(&quot;producer is making baozi &#123;0&#125;&quot;.format(n)) con.send(n) # 唤醒生成器，并且向生成器传值 con2.send(n) n += 1 if __name__ == &apos;__main__&apos;: con = consumer(&quot;c1&quot;) # 创建一个生成器c1,但是不会开始执行 con2 = consumer(&quot;c2&quot;) # 创建一个生产器C2,但是不会开始执行 p = producer() send有两个作用？1.唤醒生产器2.给yield传一个值，就是yield接收到的这个值。这个说明yield在被唤醒的时候可以接收数据。 怎么实现我们的单线程实现并发的效果呢？&emsp;&emsp;遇到IO操作就切换，IO比较耗时，协程之所以能处理大并发，就是IO操作会挤掉大量的时间。没有IO操作的话，整个程序只有cpu在运算了，因为cpu很快，所以你感觉是在并发执行的。 IO操作完成了，程序什么时候切回去？&emsp;&emsp;IO操作一旦完成，我们就自动切回去。 IO是什么?&emsp;&emsp;Python中的io模块是用来处理各种类型的I/O操作流。主要有三种类型的I/O类型：文本I/O(Text I/O)，二进制I/O(Binary I/O)和原始I/O(Raw I/O)。它们都是通用类别，每一种都有不同的后备存储。属于这些类别中的任何一个的具体对象称为文件对象，其他常用的术语为流或者类文件对象。&emsp;&emsp;除了它的类别，每一种具体的流对象也具有各种功能：它仅仅允许读，或者仅仅允许写，或者既能读又能写。它也允许任意随机访问（向前或者向后寻找任何位置），或者仅仅顺序访问（例如在套接字或管道中）。&emsp;&emsp;所有的流对于提供给它们的数据的数据类型都很严格。例如，如果用一个二进制流的write（）方法写一个字符类型的数据，那么将会触发一个TypeError错误。用文本流的write()方法来写字节对象数据也是一样的，会触发该错误。手动实现切换IO&emsp;&emsp;Greenlet是python的一个C扩展，来源于Stackless python，旨在提供可自行调度的‘微线程’， 即协程。它可以使你在任意函数之间随意切换，而不需把这个函数先声明为generator12345678910111213141516from greenlet import greenlet def test1(): print(12) gr2.switch() # 切换到test2 print(34) gr2.switch() # 切换到test2 def test2(): print(56) gr1.switch() # 切换到test1 print(78) gr1 = greenlet(test1) # 启动一个协程gr2 = greenlet(test2)gr1.switch() # 切换到test1，这个switch不写的话，会无法输出打印 小结： cpu只认识线程，而不认识协程，协程是用户自己控制的，cpu根本都不知道它们的存在。 线程的上下文切换保存在cpu的寄存器中，但是协程拥有自己的寄存上下文和栈。 协程是串行的，无需锁。 虽然greenlet确实用着比generator（生成器）还简单了，但好像还没有解决一个问题，就是遇到IO操作，自动切换，对不对？协程遇IO操作自动切换&emsp;&emsp;接下来就说说如何遇到IO就自动切换切换，Gevent 是一个第三方库，可以轻松通过gevent实现并发同步或异步编程，在gevent中用到的主要模式是Greenlet, 它是以C扩展模块形式接入Python的轻量级协程。 Greenlet全部运行在主程序操作系统进程的内部，但它们被协作式地调度。12345678910111213141516171819202122import gevent def foo(): print(&quot;Running in foo&quot;) gevent.sleep(3) # 模仿io操作，一遇到io操作就切换 print(&quot;Explicit context switch to foo again&quot;) def bar(): print(&quot;Explicit context to bar&quot;) gevent.sleep(1) print(&quot;Implicit context switch back to bar&quot;) def fun3(): print(&quot;running fun3&quot;) gevent.sleep(0) # 虽然是0秒，但是会触发一次切换 print(&quot;running fun3 again&quot;) gevent.joinall([ gevent.spawn(foo), # 生成协程 gevent.spawn(bar), gevent.spawn(fun3)]) &emsp;&emsp;当foo遇到sleep(3)的时候，切自动切换到bar函数，执行遇到sleep(1)的时候自动切换到fun3函数，遇到sleep(0)又自动切换到foo。这个时候sleep(3)还没有执行完毕，又切换到bar的sleep(1)这边，发现又没有执行完毕，就有执行fun3这边，发现sleep(0)执行完毕，则继续执行，然后又切换到foo,发现sleep(3)又没有执行完毕，就切换到bar的sleep(1)这边，发现执行完了，有切回到foo这边，执行完毕。&emsp;&emsp;比如说你现在有50处IO，然后总共加起来串行的的话，要花100秒，但是50处IO最长的那个IO只花了5秒钟，那代表中你的这个程序就是协程最多5秒就执行完毕了。 符合下面四个条件才能称之为协程： 必须在只有一个单线程里实现并发 修改共享数据不需加锁 用户程序里自己保存多个控制流的上下文栈 一个协程遇到IO操作自动切换到其它协程 协程（gevent）并发爬网页&emsp;&emsp;上面例子gevent遇到io自动切换，现在就来实际演示协程爬虫的例子正常（串行）爬网页123456789101112131415161718192021from urllib import requestimport time def run(url): print(&quot;GET:&#123;0&#125;&quot;.format(url)) resp = request.urlopen(url) # request.urlopen()函数 用来打开网页 data = resp.read() # 读取爬到的数据 with open(&quot;url.html&quot;, &quot;wb&quot;) as f: f.write(data) print(&apos;&#123;0&#125; bytes received from &#123;1&#125;&apos;.format(len(data), url)) urls = [ &apos;http://www.163.com/&apos;, &apos;https://www.yahoo.com/&apos;, &apos;https://github.com/&apos;] time_start = time.time() # 开始时间for url in urls: run(url)print(&quot;同步cost&quot;, time.time() - time_start) # 程序执行消耗的时间 协程(gevent)爬虫(gevent并发执行)123456789101112131415161718192021222324from urllib import requestimport time,gevent def run(url): print(&quot;GET:&#123;0&#125;&quot;.format(url)) resp = request.urlopen(url) # request.urlopen()函数 用来打开网页 data = resp.read() # 读取爬到的数据 with open(&quot;url.html&quot;, &quot;wb&quot;) as f: f.write(data) print(&apos;&#123;0&#125; bytes received from &#123;1&#125;&apos;.format(len(data), url)) urls = [ &apos;http://www.163.com/&apos;, &apos;https://www.yahoo.com/&apos;, &apos;https://github.com/&apos;] time_start = time.time() # 开始时间gevent.joinall([ gevent.spawn(run,&apos;http://www.163.com/&apos;), # 生成协程 gevent.spawn(run,&apos;https://www.yahoo.com/&apos;), gevent.spawn(run,&apos;https://github.com/&apos;)])print(&quot;异步cost&quot;, time.time() - time_start) # 程序执行消耗的时间对比1、2爬网页的例子，发现执行耗费时间上并没有得到明显提升，并没有并发爬网页的神奇快感，其实主要是因为gevent现在检测不到urllib的IO操作。它都不知道urllib进行了IO操作，感受不到阻塞，它都不会进行切换，所以它就串行了。 打个补丁，告诉gevent,urllib正在进行IO操作通过导入monkey模块，来打这个补丁，原代码不变，就添加一行monkey.patch_all()即可。123456789101112131415161718192021222324252627from urllib import requestimport gevent,timefrom gevent import monkey # 导入monkey模块 monkey.patch_all() # 把当前程序的所有的IO操作给作上标记 def run(url): print(&quot;GET:&#123;0&#125;&quot;.format(url)) resp = request.urlopen(url) # request.urlopen()函数 用来打开网页 data = resp.read() # 读取爬到的数据 with open(&quot;url.html&quot;, &quot;wb&quot;) as f: f.write(data) print(&apos;&#123;0&#125; bytes received from &#123;1&#125;&apos;.format(len(data), url)) urls = [ &apos;http://www.163.com/&apos;, &apos;https://www.yahoo.com/&apos;, &apos;https://github.com/&apos;] time_start = time.time() # 开始时间gevent.joinall([ # 用gevent启动协程 gevent.spawn(run, &apos;http://www.163.com/&apos;), # 第二个值是传入参数，之前我们没有讲，因为前面没有传参 gevent.spawn(run, &apos;https://www.yahoo.com/&apos;), gevent.spawn(run, &apos;https://github.com/&apos;),])print(&quot;异步cost&quot;, time.time() - time_start) # 程序执行消耗的时间时间会受到网络状态的影响&emsp;&emsp;通过打补丁来检测urllib，它就把urllib里面所有涉及到的有可能进行IO操作的地方直接花在前面加一个标记，这个标记就相当于gevent.sleep()，所以把urllib变成一个一有阻塞，它就切换了 gevent实现单线程下的多socket并发server端1234567891011121314151617181920212223242526import sys,gevent,socket,timefrom gevent import socket,monkeymonkey.patch_all() def server(port): s = socket.socket() s.bind((&apos;0.0.0.0&apos;, port)) s.listen(500) while True: cli, addr = s.accept() gevent.spawn(handle_request, cli) #协程 def handle_request(conn): try: while True: data = conn.recv(1024) print(&quot;recv:&quot;, data) conn.send(data) if not data: conn.shutdown(socket.SHUT_WR) except Exception as ex: print(ex) finally: conn.close()if __name__ == &apos;__main__&apos;: server(8888) client端123456789101112import socket HOST = &apos;localhost&apos; # The remote hostPORT = 8888 # The same port as used by the servers = socket.socket(socket.AF_INET, socket.SOCK_STREAM)s.connect((HOST, PORT))while True: msg = bytes(input(&quot;&gt;&gt;:&quot;),encoding=&quot;utf8&quot;) s.sendall(msg) data = s.recv(1024) print(&apos;Received&apos;, repr(data))s.close()","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"python-多进程","date":"2018-09-15T15:15:10.000Z","path":"2018/09/15/Python-多进程/","text":"&emsp;&emsp;进程之间是相互独立的，进程没有GIL锁，而且不存在锁的概念，进程之间的数据式不能共享的，而线程是可以的。 多进程进程的定义&emsp;&emsp;用muliprocessing这个包中的Process来定义多进程，跟定义多线程类似12345678910111213141516from multiprocessing import Process # 导入进程模块import time def run(name): time.sleep(2) print(&quot;hello&quot;, name) if __name__ == &quot;__main__&quot;: p_obj_list = list() # 存放进程对象 for i in range(10): # 启动10个进程 p = Process(target=run, args=(&quot;QQ&#123;0&#125;&quot;.format(i),)) # 产生一个进程实例 p.start() # 启动进程 p_obj_list.append(p) for p in p_obj_list: p.join() # 等待进程结果 进程中加入线程123456789101112131415161718192021from multiprocessing import Processimport time,threading def thread_run(name): # 定义线程执行的方法 print(&quot;&#123;0&#125;:&#123;1&#125;&quot;.format(name, threading.get_ident())) # thread.get_ident ()返回当前线程的标识符，标识符是一个非零整数 def run(name): time.sleep(2) print(&quot;hello&quot;, name) t = threading.Thread(target=thread_run, args=(name,)) # 嵌入线程 t.start() # 执行线程 if __name__ == &quot;__main__&quot;: p_obj_list = list() for i in range(10): p = Process(target=run, args=(&quot;QQ&#123;0&#125;&quot;.format(i),)) p.start() p_obj_list.append(p) for p in p_obj_list: p.join() 父子进程每个子进程都是由一个父进程启动的，每个程序也是有一个父进程12345678910111213141516171819from multiprocessing import Processimport os def info(title): print(title) print(&apos;module name:&apos;, __name__) print(&apos;parent process:&apos;, os.getppid()) # 获得父进程ID print(&apos;process id:&apos;, os.getpid()) # 获得子进程ID print(&apos;\\n&apos;) def f(name): info(&apos;function f&apos;) print(&apos;hello&apos;, name) if __name__ == &apos;__main__&apos;: info(&apos;main process line&apos;) p = Process(target=f, args=(&apos;QQ&apos;,)) p.start() p.join() 进程间数据交互与共享&emsp;&emsp;知道不同进程之间内存是不共享的，要想实现两个进程间的通信需要用到multiprocessing库中的queue（队列）模块，这个multiprocessing库中的queue模块跟单纯的queue库是不一样的。进程导入前者（这里的queue是专门为进程之间的通信设计的）不出错，导入后者（这里的queue主要是线程间数据交互）出错。 线程访问queue1234567891011import queue,threadingdef f(q): q.put([66, None, &apos;hello word&apos;])if __name__ == &apos;__main__&apos;: q = queue.Queue() # 把这个q传给了子线程 p = threading.Thread(target=f, args=(q,)) # 子线程访问父线程的q p.start() print(q.get()) p.join() 进程访问queue123456789101112from multiprocessing import Processimport queuedef f(q): q.put([66, None, &apos;hello word&apos;])if __name__ == &apos;__main__&apos;: q = queue.Queue() # 把这个q传给了子线程 p = Process(target=f, args=(q,)) # 子线程访问父线程的q p.start() print(q.get()) p.join() 进程访问multiprocessing库中的Queue模块1234567891011from multiprocessing import Process,Queue def f(q): q.put([66, None, &apos;hello word&apos;]) if __name__ == &apos;__main__&apos;: q = Queue() # 把这个q传给了子线程 p = Process(target=f, args=(q,)) # 子线程访问父线程的q p.start() print(q.get()) p.join() &emsp;&emsp;父进程相当于克隆一个Q，把自己的Q克隆了一份交给子进程，子进程这个时候往Q里面放了一份数据，然后父进程又能实际的获取到。但是你克隆了一份是不是就和父进程没有关系了，为什么还能联系在一起呢？但是实际上：等于这两个Q里面的数据又把它序列化了，序列化到一个中间的地方，类似于翻译，然后反序列化给这个父进程这边来了，其实这两个Q就是通过pickle来序列化的，不是一个真正的Q。小结：两个线程之间可以修改一个数据，不加锁，可能就会出错。现在进程中的Queue，是实现了数据的传递，不是在修改同一份数据，只是实现一个进程的数据传给了另外一个进程。 Pipe()实现进程间的数据交互，manger实现数据共享&emsp;&emsp;上面的例子是通过进程中的Queue，来进行数据共享的，其实还有一种方式实现数据共享，那就是管道，pipe，以及数据共享manger。 Pipe()函数&emsp;&emsp;管道函数会返回由管道双方连接的一组连接对象，该管道默认是双向的(双向的)。1234567891011121314from multiprocessing import Process, Pipe def f(conn): conn.send([66, None, &apos;hello,word&apos;]) # 发送消息给父进程 conn.send([66, None, &apos;hello,word2&apos;]) # 发送消息给父进程 conn.close() if __name__ == &apos;__main__&apos;: parent_conn, child_conn = Pipe() # 管道生成返回两个实例，是双向的，这边把第1个作为父连接，第2个作为子连接。也可以，两者角色调换一下 p = Process(target=f, args=(child_conn,)) p.start() print(parent_conn.recv()) # 接收子进程的消息 print(parent_conn.recv()) # 接收子进程的消息 p.join()如果父进程在接收,但是子进程没有发,那么父进程就会一直等待下去 manger()&emsp;&emsp;manger可以完成数据间的共享。12345678910111213141516171819202122from multiprocessing import Process, Managerimport os def f(d, l): d[os.getpid()] = os.getpid() l.append(os.getpid()) print(l) if __name__ == &apos;__main__&apos;: with Manager() as manager: d = manager.dict() # 声明一个字典，这个字典是用manger声明的，不是用dict()声明的 # manger.dict()是用专门的语法生产一个可在多进程之间进行传递和共享的一个字典 l = manager.list(range(5)) # 同样声明一个列表 p_list = [] for i in range(10): p = Process(target=f, args=(d, l)) p.start() p_list.append(p) for res in p_list: res.join() print(d) print(l)线程修改同一份数据的时候需要加锁，进程修改数据呢：不用加锁，因为这个manger已经帮你加锁了，它就默认不允许两个进程同时修改一份数据。两个进程没有办法同时修改一份数据，进程之间是独立的，它自己也要加锁，因为它把自己的东西同时copy好几份，跟刚刚的那个Queue一样，copy10个字典最终合成一个字典 进程锁和进程池进程锁&emsp;&emsp;通过multiprocessing中的Lock模块来实现进程锁1234567891011from multiprocessing import Process,Lock # 导入进程锁 def f(l, i): l.acquire() # 加锁 print(&quot;hello word&quot;, i) l.release() # 释放锁 if __name__ == &quot;__main__&quot;: lock = Lock() # 定义锁 for num in range(10): Process(target=f, args=(lock, num,)).start() # 把锁传入进程中进程中不是相互独立的吗？为什么还要加锁：虽然每个进程都是独立运行的，但是问题来了，它们共享一块屏幕。这个锁存在的意义就是屏幕共享。如果进程1想着打印数据，而进程2想也想打印数据的情况，就有可能乱套了，然后通过这个锁来控制，去打印的时候，这个屏幕只有我独占，导致屏幕不会乱。 进程池apply和apply_sayncappley&emsp;&emsp;同步执行，也就是串行执行的1234567891011121314from multiprocessing import Pool # 导入进程池模块poolimport time,os def foo(i): time.sleep(2) print(&quot;in process&quot;, os.getpid()) # 打印进程号 if __name__ == &quot;__main__&quot;: pool = Pool(processes=5) # 设置进程池个数为5，也可以写成pool = Pool(5)，允许进程池同时放入5个进程，并且这5个进程交给cpu去运行 for i in range(10): pool.apply(func=foo, args=(i,)) # 同步执行挂起进程 print(&apos;end&apos;) pool.close() pool.join() # 进程池中进程执行完毕后再关闭，如果注释，那么程序直接关闭。一个一个打印 apply_saync&emsp;&emsp;异步执行，也就是并行执行。1234567891011121314from multiprocessing import Pool # 导入进程池模块poolimport time,os def foo(i): time.sleep(2) print(&quot;in process&quot;, os.getpid()) # 打印进程号 if __name__ == &quot;__main__&quot;: pool = Pool(processes=5) # 设置进程池个数为5，也可以写成pool = Pool(5)，允许进程池同时放入5个进程，并且这5个进程交给cpu去运行 for i in range(10): pool.apply_async(func=foo, args=(i,)) # 采用异步方式执行foo函数 print(&apos;end&apos;) pool.close() pool.join() # 进程池中进程执行完毕后再关闭，如果注释，那么程序直接关闭。 异步下回调函数&emsp;&emsp;程序执行完毕之后，再回调过来执行这个Bar函数。12345678910111213141516171819from multiprocessing import Process,Poolimport time,os def foo(i): time.sleep(2) print(&quot;in process&quot;, os.getpid()) # 打印子进程的进程号 return i def bar(arg): print(&apos;--&gt;exec done:&apos;, arg, os.getpid()) # 打印进程号 if __name__ == &quot;__main__&quot;: pool = Pool(processes=2) print(&quot;主进程&quot;, os.getpid()) # 主进程的进程号 for i in range(3): pool.apply_async(func=foo, args=(i,), callback=bar) # 执行回调函数callback=Bar,在主进程中执行 print(&apos;end&apos;) pool.close() pool.join() # 进程池中进程执行完毕后再关闭，如果注释，那么程序直接关闭。五个五个打印回调函数在主进程执行,传入的参数来自子进程 回调函数说明fun=Foo干不完就不执行bar函数，等Foo执行完就去执行Bar 这个回调函数是主进程去调用的，而不是每个子进程去调用的。 回调函数的用处： 比如说你从各个机器上备份完毕，在回调函数中自动写一个脚本，说备份完毕 回调函数是主进程调用的原因？ 如果是子进程去调用这个回调函数，有多少个子进程就有多少个连接，如果是主进程的话，只需要一次长连接就可以了，这个效率就高了","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"python-线程与进程","date":"2018-09-12T11:37:08.000Z","path":"2018/09/12/Python-线程与进程/","text":"&emsp;&emsp;计算机所有的指令的操作都是有CPU来负责的，cpu是来负责运算的。OS(操作系统) 调度cpu的最小单位就是线程。&emsp;&emsp;进程：是以一个整体的形式暴露给操作系统管理，里面包含对各种资源的调用，内存的管理，网络接口的调用等等，是各种资源管理的集合&emsp;&emsp;线程：是操作系统的最小的调度单位，是一串指令的集合。 进程(Process）&emsp;&emsp;程序并不能单独运行，只有将程序装载到内存中，系统为它分配资源才能运行，而这种执行的程序就称之为进程。程序和进程的区别就在于：程序是指令的集合，它是进程运行的静态描述文本；进程是程序的一次执行活动，属于动态概念。&emsp;&emsp;在多道编程中，我们允许多个程序同时加载到内存中，在操作系统的调度下，可以实现并发地执行。这是这样的设计，大大提高了CPU的利用率。进程的出现让每个用户感觉到自己独享CPU，因此，进程就是为了在CPU上实现多道编程而提出的。 线程(Thead)&emsp;&emsp;线程是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。 有了进程为什么还要线程?&emsp;&emsp;进程有很多优点，它提供了多道编程，让我们感觉我们每个人都拥有自己的CPU和其他资源，可以提高计算机的利用率。很多人就不理解了，既然进程这么优秀，为什么还要线程呢？其实，仔细观察就会发现进程还是有很多缺陷的，主要体现在两点上： 进程只能在一个时间干一件事，如果想同时干两件事或多件事，进程就无能为力了。 进程在执行的过程中如果阻塞，例如等待输入，整个进程就会挂起，即使进程中有些工作不依赖于输入的数据，也将无法执行。 &emsp;&emsp;例如，我们在使用qq聊天， qq做为一个独立进程如果同一时间只能干一件事，那他如何实现在同一时刻：即能监听键盘输入、又能监听其它人给你发的消息、同时还能把别人发的消息显示在屏幕上呢？你会说，操作系统不是有分时么？但是分时是指在不同进程间的分时， 即操作系统处理一会你的qq任务，又切换到word文档任务上了，每个cpu时间片分给你的qq程序时，事实上，你的qq还是同一时间只能干一件事情。&emsp;&emsp;再直白一点， 一个操作系统就像是一个工厂，工厂里面有很多个生产车间，不同的车间生产不同的产品，每个车间就相当于一个进程，且你的工厂又穷，供电不足，同一时间只能给一个车间供电，为了能让所有车间都能同时生产，你的工厂的电工只能给不同的车间分时供电，但是轮到你的qq车间时，发现只有一个干活的工人，结果生产效率极低，为了解决这个问题，就需要多加几个工人，让几个人工人并行工作，这每个工人，就是线程！ 进程和线程的区别 线程是共享内存空间的；进程的内存是独立的。 线程可以直接访问此进程中的数据部分；进程有他们独立拷贝自己父进程的数据部分，每个进程是独立的 同一进程的线程之间直接交流(直接交流涉及到数据共享，信息传递)；两个进程想通信，必须通过一个中间代理来实现。 创建一个新的线程很容易；创建新的进程需要对其父进程进行一次克隆。 一个线程可以控制和操作同一进程里的其他线程；但是进程只能操作子进程。 对主线程的修改，可能会影响到进程中其他线程的修改；对于一个父进程的修改不会影响其他子进程(只要不删除父进程即可)概念小结 线程是操作系统最小的调度单位，是一串指令的集合。 进程要操作CPU，必须先创建一个线程。 进程本身是不可以执行的，操作cpu是通过线程实现的，因为它是一堆执行，而进程是不具备执行概念的。就像一个屋子，屋子就是进程，但是屋子里面的每一个人就是线程，屋子就是内存空间。 单核CPU只能同时干一件事，但是为什么给我们的感觉是在干了很多件事？因为CPU太快了，可以有N多次切换。 进程是通过PID来区分的，并不是通过进程名来区分的。进程里面的第一个线程就是主线程，父线程和子线程是相互独立的，只是父线程创建了子线程，父线程down了，子线程不会受到影响的。 主线程修改会影响其他线程，因为它们是共享数据的。 线程启动比进程块，但是运行速度没有可比性 threading.Thread模块函数式多线程1234567891011121314151617import threading,time def run(n): print(&quot;run&quot;, n) time.sleep(2) print(n,&apos;end time:&apos;,time.time())&quot;&quot;&quot;第一个参数是线程函数变量，第二个参数args是一个元组变量参数，如果只传递一个值，就只需要i,如果需要传递多个参数，那么还可以继续传递下去其他的参数，其中的逗号不能少，少了逗号位置参数指引就会出错。&quot;&quot;&quot; t1 = threading.Thread(target=run, args=(&quot;t1&quot;,)) # 生成线程对象t2 = threading.Thread(target=run, args=(&quot;t2&quot;,))print(&apos;start:&apos;,time.time())t1.start() # start()函数启动一个线程t2.start() 继承式多线程1234567891011121314151617181920import threading,time class MyThread(threading.Thread): # 继承threading.Thread &quot;&quot;&quot;继承式多线程&quot;&quot;&quot; def __init__(self, n): super(MyThread,self).__init__() # 也可以写成这样threading.Thread.__init__(self) self.n = n def run(self): # 重写run方法 &quot;&quot;&quot;这个方法不能叫别的名字，只能叫run方法&quot;&quot;&quot; print(&quot;run&quot;, self.n) time.sleep(2) print(self.n,&apos;end time:&apos;,time.time()) t1 = MyThread(&quot;t1&quot;) # 实例化t2 = MyThread(&quot;t2&quot;)print(&apos;start:&apos;,time.time()) t1.start() # 启动一个多线程t2.start() &amp;emsp；&amp;emsp；在上面两个示例代码中，都包含一个主线程和两个子线程，主线程在启动子线程后，子线程就是独立的，所以主线程不会等待子线程的sleep就直接运行下去。如果实现等待线程执行结果可以使用join。 join函数12345678910111213141516171819202122import threading,time class MyThread(threading.Thread): # 继承threading.Thread &quot;&quot;&quot;继承式多线程&quot;&quot;&quot; def __init__(self, n): super(MyThread,self).__init__() # 也可以写成这样threading.Thread.__init__(self) self.n = n def run(self): # 重写run方法 &quot;&quot;&quot;这个方法不能叫别的名字，只能叫run方法&quot;&quot;&quot; print(&quot;run&quot;, self.n) time.sleep(2) print(self.n,&apos;end time:&apos;,time.time()) t1 = MyThread(&quot;t1&quot;) # 实例化t2 = MyThread(&quot;t2&quot;)print(&apos;start:&apos;,time.time()) t1.start() # 启动一个多线程t1.join()t2.start()print(&apos;main end time:&apos;,time.time()) 加了join之后，主线程依赖子线程执行完毕才往下走。 如果想要的是线程依然是并行效果，就需要更换join()的位置 计算多线程执行时间12345678910111213141516171819import threading,time def run(n): # 这边的run方法的名字是自行定义的，跟继承式多线程不一样，那个是强制的 print(&quot;task:&quot;, n) time.sleep(2) print(&quot;task done&quot;, n) start_time = time.time() # 开始时间t_obj = [] # 存放子线程实例for i in range(10): # 一次性启动10个线程 t = threading.Thread(target=run, args=(&quot;t-&#123;0&#125;&quot;.format(i),)) t.start() t_obj.append(t) # 为了不阻塞后面线程的启动，不在这里join，先放到一个列表中 for t in t_obj: # 循环线程实例列表，等待所有线程执行完毕 t.join() print(&quot;--------all thread has finished&quot;)print(&quot;cost:&quot;, time.time() - start_time) # 计算总耗时 守护进程&emsp;&emsp;只要主线程执行完毕，它不管子线程有没有执行完毕，就退出。现在就可以把所有的子线程变成守护线程。子线程变成守护线程之后，主程序就不会等子线程结束再退出了。说白了，设置一个主人，在设置几个仆人，这几个仆人都是为主人服务的。可以帮主人做很多事情，一个主人（主线程）可以有多个仆人（守护线程），服务的前提是，主线程必须存在，如果主线程不存在，则守护进程也没了。那守护进程是干嘛的呢？可以管理一些资源，打开一些文件，监听一些端口，监听一些资源，把一些垃圾资源回收，可以干很多事情，可以随便定义。123456789101112131415import threading,timedef run(n): print(&quot;task:&quot;, n) time.sleep(2) print(&quot;task done&quot;, n) start_time = time.time()for i in range(5): t = threading.Thread(target=run,args=(&quot;t-&#123;0&#125;&quot;.format(i),)) t.setDaemon(True) # Daemon意思是守护进程，这边是把当前线程设置为守护线程 t.start() print(&quot;--------all thread has finished&quot;)print(&quot;cost:&quot;, time.time() - start_time)守护进程一定要在start之前设置，start之后就不能设置了，之后设置会报错 使用场景&emsp;&emsp;比如写一个socket_server，每一个链接过来，socket_server就会给这个链接分配一个新的线程。如果我手动的把socket_server停掉。那这种情况你必须手动停掉服务，那它就要down了，这种情况下还要等线程结束吗？就不用等线程结束了，它自己就直接结束了。这样，是不是就可以把每个socket线程设置一个守护线程，主线程一旦down掉，就全部退出。 补充 theading.current_thead()查看当前线程； 用theading.active_count()来统计当前活动的线程数 线程个数=子线程数+主线程数 GIL锁(全局解释器锁)&emsp;&emsp;计算机有4核，代表着同一时间，可以干4个任务。如果单核cpu的话，我启动10个线程，我看上去也是并发的，因为是执行了上下文的切换，让看上去是并发的。但是单核永远肯定时串行的，它肯定是串行的，cpu真正执行的时候，因为一会执行1，一会执行2.。。。。正常的线程就是这个样子的。但是，在python中，无论有多少核，永远都是假象。无论是4核，8核，还是16核…….不好意思，同一时间执行的线程只有一个(线程)，它就是这个样子的。这个是python的一个开发时候，设计的一个缺陷，所以说python中的线程是假线程。 GIL存在的意义&emsp;&emsp;在新处理器上运行的程序要想充分利用其性能，必须按照并发方式进行重写。大部分开发者听到“并发”通常会立刻想到多线程的程序。目前来说，多线程执行还是利用多核系统最常用的方式。尽管多线程编程大大好于“顺序”编程，不过即便是仔细的程序员也没法在代码中将并发性做到最好。编程语言在这方面应该做的更好，大部分应用广泛的现代编程语言都会支持多线程编程。&emsp;&emsp;要想利用多核系统，Python必须支持多线程运行。作为解释型语言，Python的解释器必须做到既安全又高效。我们都知道多线程编程会遇到的问题。解释器要留意的是避免在不同的线程操作内部共享的数据。同时它还要保证在管理用户线程时保证总是有最大化的计算资源。&emsp;&emsp;那么，不同线程同时访问时，数据的保护机制是怎样的呢？答案是解释器全局锁。从名字上看能告诉我们很多东西，很显然，这是一个加在解释器上的全局（从解释器的角度看）锁（从互斥或者类似角度看）。这种方式当然很安全，但是它有一层隐含的意思（Python初学者需要了解这个）：对于任何Python程序，不管有多少的处理器，任何时候都总是只有一个线程在执行。 GIL锁关系图&emsp;&emsp;GIL(全局解释器锁)是加在python解释器里面的，效果如图：总结:&emsp;&emsp;需要明确的一点是GIL并不是Python的特性，它是在实现Python解析器(CPython)时所引入的一个概念。就好比C++是一套语言（语法）标准，但是可以用不同的编译器来编译成可执行代码。有名的编译器例如GCC，INTEL C++，Visual C++等。Python也一样，同样一段代码可以通过CPython，PyPy，Psyco等不同的Python执行环境来执行。像其中的JPython就没有GIL。然而因为CPython是大部分环境下默认的Python执行环境。所以在很多人的概念里CPython就是Python，也就想当然的把GIL归结为Python语言的缺陷。所以这里要先明确一点：GIL并不是Python的特性，Python完全可以不依赖于GIL。 CPython：是用C语言实现Pyhon，是目前应用最广泛的解释器。 线程锁（互斥锁）1234567891011121314151617181920212223import threading,time def run(n): global num # 把num变成全局变量 time.sleep(1) # 注意了sleep的时候是不占有cpu的，这个时候cpu直接把这个线程挂起了，此时cpu去干别的事情去了 num += 1 # 所有的线程都做+1操作 num = 0 # 初始化num为0t_obj = list()for i in range(100): t = threading.Thread(target=run, args=(&quot;t-&#123;0&#125;&quot;.format(i),)) t.start() t_obj.append(t) for t in t_obj: t.join() print(&quot;--------all thread has finished&quot;)print(&quot;num:&quot;, num) # 输出最后的num值#执行结果--------all thead has finished(&apos;num:&apos;, 97) #输出的结果 &emsp;&emsp;其实这种情况只能在python2.x 中才会出现的，python3.x里面没有这种现象，下面我们就用一张图来解释一下这个原因。如图：解释： 到第5步的时候，可能这个时候python正好切换了一次GIL(据说python2.7中，每100条指令会切换一次GIL),执行的时间到了，被要求释放GIL,这个时候thead 1的count=0并没有得到执行，而是挂起状态，count=0这个上下文关系被存到寄存器中. 然后到第6步，这个时候thead 2开始执行，然后就变成了count = 1,返回给count，这个时候count=1. 然后再回到thead 1，这个时候由于上下文关系，thead 1拿到的寄存器中的count = 0，经过计算，得到count = 1，经过第13步的操作就覆盖了原来的count = 1的值，所以这个时候count依然是count = 1，所以这个数据并没有保护起来。 添加线程锁&emsp;&emsp;通过上面的图我们知道，结果依然是不准确的。所以我还要加一把锁，这个是用户级别的锁。12345678910111213141516171819202122import threading,timedef run(n): lock.acquire() # 添加线程锁 global num # 把num变成全局变量 time.sleep(0.1) # 注意了sleep的时候是不占有cpu的，这个时候cpu直接把这个线程挂起了，此时cpu去干别的事情去了 num += 1 # 所有的线程都做+1操作 lock.release() # 释放线程锁 num = 0 # 初始化num为0lock = threading.Lock() # 生成线程锁实例t_obj = list()for i in range(10): t = threading.Thread(target=run, args=(&quot;t-&#123;0&#125;&quot;.format(i),)) t.start() t_obj.append(t) for t in t_obj: t.join() # 为join是等子线程执行的结果，如果不加，主线程执行完，下面就获取不到子线程num的值了，共享数据num值就错误了print(&quot;--------all thread has finished&quot;)print(&quot;num:&quot;, num) # 输出最后的num值小结： 用theading.Lock()创建一个lock的实例。 在线程启动之前通过lock.acquire()加加锁，在线程结束之后通过lock.release()释放锁。 这层锁是用户开的锁，就是我们用户程序的锁。跟我们这个GIL没有关系，但是它把这个数据相当于copy了两份，所以在这里加锁，以确保同一时间只有一个线程，真真正正的修改这个数据，所以这里的锁跟GIL没有关系，你理解就是自己的锁。 加锁，说明此时我来去修改这个数据，其他人都不能动。然后修改完了，要把这把锁释放。这样的话就把程序编程串行了。 死锁&emsp;&emsp;在线程间共享多个资源的时候，如果两个线程分别占有一部分资源并且同时等待对方的资源，就会造成死锁，因为系统判断这部分资源都正在使用，所有这两个线程在无外力作用下将一直等待下去.（大锁内加小锁）1234567891011121314151617181920212223242526272829303132333435import threading,time mutexA=threading.Lock()mutexB=threading.Lock()class MyThread(threading.Thread): def run(self): self.func1() self.func2() def func1(self): print(&apos;%s func1 start&apos;%self.name) mutexA.acquire() print(&apos;%s func1 拿到A锁 &apos;%self.name) mutexB.acquire() print(&apos;%s func1 拿到B锁 &apos;%self.name) mutexB.release() mutexA.release() print(&apos;%s func1 end&apos;%self.name) def func2(self): print(&apos;%s func2 start&apos;%self.name) mutexB.acquire() print(&apos;%s func2 拿到B锁 &apos;%self.name) time.sleep(2) mutexA.acquire() print(&apos;%s func2 拿到A锁 &apos;%self.name) mutexA.release() mutexB.release() print(&apos;%s func2 end&apos;%self.name)if __name__ == &apos;__main__&apos;: for i in range(5): t=MyThread() t.start()线程1拿到B锁，线程3拿到A锁，造成同时等待，而且线程2、4、5也会因为拿不到锁等待。 递归锁（RLock）&emsp;&emsp;这个RLock内部维护着一个Lock和一个counter变量，counter记录了acquire的次数，从而使得资源可以被多次require。直到一个线程所有的acquire都被release，其他的线程才能获得资源。12345678910111213141516171819202122232425262728293031323334import threading,time mutexA=mutexB=threading.RLock()class MyThread(threading.Thread): def run(self): self.func1() self.func2() def func1(self): print(&apos;%s func1 start&apos;%self.name) mutexA.acquire() print(&apos;%s func1 拿到A锁 &apos;%self.name) mutexB.acquire() print(&apos;%s func1 拿到B锁 &apos;%self.name) mutexB.release() mutexA.release() print(&apos;%s func1 end&apos;%self.name) def func2(self): print(&apos;%s func2 start&apos;%self.name) mutexB.acquire() print(&apos;%s func2 拿到B锁 &apos;%self.name) time.sleep(2) mutexA.acquire() print(&apos;%s func2 拿到A锁 &apos;%self.name) mutexA.release() mutexB.release() print(&apos;%s func2 end&apos;%self.name)if __name__ == &apos;__main__&apos;: for i in range(5): t=MyThread() t.start()&emsp;&emsp;由于锁A，B是同一个递归锁，thread1拿到A,B锁，counter记录了acquire的次数2次，然后在func1执行完毕，就释放递归锁，在thread1释放完递归锁，执行完func1代码，接下来会有2种可能：1、thread1在次抢到递归锁，执行func2代码 2、其他的线程抢到递归锁，去执行func1的任务代码递归锁用于多重锁的情况，如果只是一层锁，就用不上递归锁递归锁原理其实很简单：就是每开一把门，在字典里面存一份数据，退出的时候去到door1或者door2里面找到这个钥匙退出，如图： 信号量（Semaphore）&emsp;&emsp;之前讲的线程锁（互斥锁）同时只允许一个线程更改数据，而Semaphore是同时允许一定数量的线程更改数据 ，比如厕所有3个坑，那最多只允许3个人上厕所，后面的人只能等里面有人出来了才能再进去。 信号量是一个变量，控制着对公共资源或者临界区的访问。信号量维护着一个计数器，指定可同时访问资源或者进入临界区的线程数。 每次有一个线程获得信号量时，计数器-1。若计数器为0，其他线程就停止访问信号量，直到另一个线程释放信号量 1234567891011121314151617import threading,time def run(n): semaphore.acquire() # 加信号量锁 time.sleep(5) print(&quot;run the thread: %s\\n&quot; % n) semaphore.release() # 释放信号量锁 if __name__ == &apos;__main__&apos;: semaphore = threading.BoundedSemaphore(5) # 最多允许5个线程同时运行(Bounded:绑定，Semaphore：信号量) for i in range(20): t = threading.Thread(target=run, args=(i,)) t.start()while threading.active_count() != 1: passelse: print(&apos;----all threads done---&apos;) 上面程序的执行，会让人感觉是：分了4组，前5个同时完成，然后又5个同时进去。但是实际的效果是：这5个里面如果有3个完成，就会立刻再放3个进去。不会等5个都完成，每出来1个就放进去1个，出来几个放进去几个使用场景和总结 连接池，线程池，MySQL的有连接池，同一时间有多少个并发，就能连多少个连接。 我们为了保证我的socket_server，因为python不会默认现在你启动多少个线程，但是你启动的线程越多，就会把系统拉的越慢，就会把程序拉的越慢。这里就可以搞一个我同一时间放100线程个进来，就是用semaphore event&emsp;&emsp;事件处理的机制：全局定义了一个“Flag”，如果“Flag”值为 False，那么当程序执行 event.wait 方法时就会阻塞，如果“Flag”值为True，那么event.wait 方法时便不再阻塞。threading.Event 实现线程间通信,使用threading.Event可以使一个线程等待其他线程的通知 event = threading.Event() # 设置一个事件的全局变量 event.is_set() # 判断是否已经设置标志位。 event.wait() # 没有设置标志位的时候会阻塞，一遇到标志位就不会阻塞 #判断是否已经设置标志位。 event.set() # 设置标志位 ，标志位设置了，代表着绿灯，直接通行。 event.clear() # 清除标志位，标志位被清空，代表红灯，wait等待变绿灯。 1234567891011121314151617181920212223242526272829303132333435import threading,time event = threading.Event() # 生成线程事件实例 def lighter(): count = 0 event.set() # 先设置标志位,代表绿灯 while True: if count &gt; 5 and count &lt; 10: # 改成红灯 event.clear() # 清除标志位，变成红灯 print(&quot;red light is on ....&quot;) elif count &gt; 10: event.set() # 创建标志位，变成绿灯 count = 0 else: print(&quot;green light is on ....&quot;) time.sleep(1) count += 1 def car(name): while True: if event.is_set(): # 有标志位，代表是绿灯 print(&quot;&#123;0&#125; running ....&quot;.format(name)) time.sleep(1) else: # 如果不是绿灯就代表红灯 print(&quot;&#123;0&#125; sees red light ,waiting ....&quot;.format(name)) event.wait() # 阻塞 print(&quot;green light is on , start going ...&quot;) light = threading.Thread(target=lighter,) # 启动代表红绿灯的线程light.start()car1 = threading.Thread(target=car, args=(&quot;car1&quot;,)) # 启动代表车的线程car1.start()","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"Python-常用模块","date":"2018-09-12T04:25:30.000Z","path":"2018/09/12/Python-异常处理/","text":"python异常处理常见异常 异常 原因 AttributeError 试图访问一个对象没有的树形，比如foo.x，但是foo没有属性x FileNotFoundError 输入/输出异常；基本上是无法打开文件 ImportError 无法引入模块或包；基本上是路径问题或名称错误 IndexError 下标索引超出序列边界，比如当x只有三个元素，却试图访问x[5] NameError 使用一个还未被赋予对象的变量 KeyError 试图访问字典里不存在的键 KeyboardInterrupt 键盘中断 SyntaxError Python代码非法，代码不能编译 IndentationError 语法错误（的子类）:代码没有正确对齐 TypeError 传入对象类型与要求的不符合 忘记在if,elif,else,for,while,class,def声明末尾添加： =和==混淆使用 结构1234567891011121314151617try: open(&quot;tes.txt&quot;)except (KeyError,IndexError) as e : print(&quot;没有这个key&quot;,e)except IndexError as e : print(&quot;列表操作错误&quot;,e)except Exception as e: print(&quot;未知错误&quot;,e)else: print(&quot;一切正常&quot;)finally: print(&quot;不管有没有错，都执行&quot;) 自动触发异常1234try: raise Exception(&apos;错误&apos;)except Exception as e: print(e)","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"Python-常用模块","date":"2018-09-11T05:25:30.000Z","path":"2018/09/11/Python-常用模块/","text":"常用模块Time模块 时间戳: 从1970年1月1日00:00:00开始按秒计算的偏移量 元组(struct_time): 属性 值 备注 tm_year 2019 tm_mon 3 tm_mday 18 tm_hour 10 tm_min 12 tm_sec 23 tm_wday 0 0-6,0表示周日 tm_yday 77 一年中第几天 tm_isdst 0 是否是夏令时 格式化(format_time): %Y-%m-%d %H:%M:%S 格式 含义 备注 %Y 年 %m 月 %d 日 %H 时 %M 分 %S 秒 %w 周几 1234567891011121314import time# 时间戳print(time.time())# 元组print(time.gmtime())print(time.localtime())# 格式化formatx = time.localtime()print(time.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;,x))# 去格式化,转换为元组print(time.strptime(&apos;2019-03-18 12:03:54&apos;, &apos;%Y-%m-%d %H:%M:%S&apos;)) 结构化时间转换成字符串时间 对于time.localtime()的类型是&lt;class &#39;time.struct_time&#39;&gt;, 如果转换为字符串类型使用asctime()123456x = time.localtime()print(time.asctime(x))print(type(time.asctime(x)))Sat Mar 23 12:54:56 2019&lt;class &apos;str&apos;&gt; 时间戳转换为字符串时间 12345print(time.ctime())print(type(time.ctime()))Sat Mar 23 12:56:57 2019&lt;class &apos;str&apos;&gt; sleep() 延迟给定的秒数 clock() 用以浮点数计算的秒数返回当前的CPU时间。用来衡量不同程序的耗时 参考 Time模块 datetime模块datatime模块重新封装了time模块，提供更多接口，datetime.py中主要有如下几个类:|类名|功能||-|-||date|日期对象,常用的属性有year, month, day||time|时间对象,常用的属性有hour, minute, second, microsecond||datetime|日期时间对象||timedelta|时间间隔，即两个时间点之间的长度||tzinfo|时区信息对象| date类 date对象由year年份、month月份及day日期三部分构成：date（year，month，day) 123456import datetimea = datetime.date.today()print(a.year)print(getattr(a, &apos;year&apos;))print(a.__getattribute__(&apos;year&apos;)) date对象中包含的方法与属性日期比较|格式转换|字符串输出 time类 time类由hour小时、minute分钟、second秒、microsecond毫秒和tzinfo五部分组成 方法与date类似 datetime类 datetime类其实是可以看做是date类和time类的合体，其大部分的方法和属性都继承于这二个类 专属于datetime的方法和属性12345678910import datetimea = datetime.datetime.now()# a是一个datetime.datetime对象print(a)# 返回一个datetime对象的日期部分print(a.date())2019-03-23 13:58:33.6612582019-03-23 timedelta类timedelta类是用来计算二个datetime对象的差值的。12d1 = datetime.datetime.now()d2 = d1 + datetime.timedelta(hours = 8) 参考 Python datetime模块详解 random模块123random.randrange(1,7) # 随机整数,不包括7random.randint(1,7) # 实现也是调用randrange,不过b+1,所以包括7random.choice(&apos;hello&apos;) # 获取随机的一个元素 获取随机长度字符串1234567def random_str(random_length=8): str = &apos;&apos; Chars = &apos;AaBbCcDdEeFfGgHhIiJjKkLlMmNnOoPpQqRrSsTtUuVvWwXxYyZz01234567890&apos; lens = len(Chars) - 1 for i in range(random_length): str += Chars[random.randint(0,lens)] return str os模块12345678910111213141516171819202122232425os.environ #获取系统环境变量# 路径拼接print(os.path.join(r&quot;C:\\article_spider\\utils&quot;, &quot;study_python.py&quot;))&gt;&gt;&gt; C:\\article_spider\\utils\\study_python.py# 如果path存在，返回True；如果path不存在，返回Falseprint(os.path.exists(r&apos;C:\\Users\\HDU-rib\\Documents\\Python\\ArticleSpider\\article_spider\\article_spider\\utils&apos;))# 测试print(os.path.abspath(__file__)) #返回path规范化的绝对路径print(os.path.split(__file__)) #按照路径将路径和文件名分割开print(os.path.dirname(__file__)) #返回path的目录。其实就是os.path.split(path)的第一个元素print(os.path.basename(__file__)) #返回path最后的文件名。如何path以／或\\结尾，那么就会返回空值。即os.path.split(path)的第二个元素# 获取当前工作目录，即当前python脚本工作的目录路径print(os.getcwd())print(os.path.dirname(os.path.abspath(__file__)))C:\\Users\\HDU-rib\\Documents\\Python\\ArticleSpider\\article_spider\\article_spider\\utils\\study_python.py(&apos;C:/Users/HDU-rib/Documents/Python/ArticleSpider/article_spider/article_spider/utils&apos;, &apos;study_python.py&apos;)C:/Users/HDU-rib/Documents/Python/ArticleSpider/article_spider/article_spider/utilsstudy_python.pyC:\\Users\\HDU-rib\\Documents\\Python\\ArticleSpider\\article_spider\\article_spider\\utilsC:\\Users\\HDU-rib\\Documents\\Python\\ArticleSpider\\article_spider\\article_spider\\utils 正斜杠与反斜杠12345678910path = r&quot;C:\\Windows\\temp\\readme.txt&quot;path1 = r&quot;c:\\windows\\temp\\readme.txt&quot;path2 = &quot;c:\\\\windows\\\\temp\\\\readme.txt&quot;path3 = &quot;c:/windows/temp/readme.txt&quot;打开文件函数open()中的参数可以是path也可以是path1、path2、path3。path：&quot;\\&quot;为字符串中的特殊字符，加上r后变为原始字符串，则不会对字符串中的&quot;\\t&quot;、&quot;\\r&quot; 进行字符串转义；path1：大小写不影响windows定位到文件；path2：用一个&quot;\\&quot;取消第二个&quot;\\&quot;的特殊转义作用，即为&quot;\\\\&quot;；path3：用正斜杠做目录分隔符也可以转到对应目录，并且在python中path3的方式也省去了反斜杠\\转义的烦恼。 sys模块123456# 退出程序,正常退出时exit(0)print(sys.exit(0))# 获取Python解释程序的版本信息print(sys.version)# 返回模块的搜索路径,初始化时使用PYTHONPATH环境变量的值print(sys.path) 123456# 从程序外部获取参数import os, sysprint(sys.argv) # 命令行参数List，第一个元素是程序本身路径&gt;&gt;&gt; python study_python.py 1 2 3[&apos;study_python.py&apos;, &apos;1&apos;, &apos;2&apos;, &apos;3&apos;] Json和pickle模块 为什么要序列化 在编程语言中，’状态’会以各种各样有结构的数据类型(也可简单的理解为变量)的形式被保存在内存中。内存是无法永久保存数据的，当程序运行了一段时间，我们断电或者重启程序，内存中关于这个程序的之前一段时间的数据（有结构）都被清空了。在断电或重启程序之前将程序当前内存中所有的数据都保存下来（保存到文件中），以便于下次程序执行能够从文件中载入之前的数据，然后继续执行，这就是序列化。 跨平台数据交互 序列化之后，不仅可以把序列化后的内容写入磁盘，还可以通过网络传输到别的机器上，如果收发的双方约定好实用一种序列化的格式，那么便打破了平台/语言差异化带来的限制，实现了跨平台数据交互。反过来，把变量内容从序列化的对象重新读到内存里称之为反序列化。 JSON和Python内置的数据类型对应如下 廖雪峰讲序列化 JSON的数据类型 12345678class Student(object): def __init__(self, name, age, score): self.name = name self.age = age self.score = scores = Student(&apos;Bob&apos;, 20, 88)print(json.dumps(s, default=lambda obj: obj.__dict__)) 在dumps的时候先回判断一下s是否是字符串,如果是直接序列化,如果不是通过函数chunks = self.iterencode(o, _one_shot=True)进行处理,而default函数就是在这里面调用,转换为dict. 序列化和编解码的区别 编码这些应该是针对字符而言的，整形及数值这些是默认二进制编码的。因此，无论是硬盘中还是内存中，虽然存的都是二进制码，但是字符的编码方式可以是utf8,unicode等等。 序列化只是一种将对象写入字节流的方法而已。可以自己去定义对象的拆分和组装，对象中的字符，写入字节流时，可以选择编码方式，其他的数值型的话，可以直接按照默认的二进制码进行序列化。 1234567891011121314151617181920d = &quot;adc中&quot;a = d.encode(&apos;utf-8&apos;)b = json.dumps(d)print(d) #编码格式unicodeprint(a) #编码格式utf-8print(b) #编码格式unicodeprint(b.encode(&apos;utf-8&apos;))print(type(d))print(type(a))print(type(b))adc中b&apos;adc\\xe4\\xb8\\xad&apos;&quot;adc\\u4e2d&quot;b&apos;&quot;adc\\\\u4e2d&quot;&apos;&lt;class &apos;str&apos;&gt;&lt;class &apos;bytes&apos;&gt;&lt;class &apos;str&apos;&gt; configparser模块configparser模块在python中用来读取配置文件，配置文件的格式跟windows下的ini配置文件相似，可以包含一个或多个节(section), 每个节可以有多个参数（键=值）。使用的配置文件的好处就是不用在程序员写死，可以使程序更灵活。 创建配置文件12345678910111213141516import configparserconfig = configparser.ConfigParser()config[&apos;default&apos;] = &#123;&apos;ServerAliveInterval&apos;:&apos;45&apos;, &apos;Compression&apos;:&apos;yes&apos;, &apos;CompressionLevel&apos;:&apos;9&apos;&#125;config[&apos;bitbucket.org&apos;] = &#123;&#125;config[&apos;bitbucket.org&apos;][&apos;User&apos;] = &apos;hg&apos;config[&apos;topsecret.server.com&apos;] = &#123;&apos;port&apos;:&apos;50022&apos;, &apos;Forwardx11&apos;:&apos;no&apos;&#125;with open(&apos;example.ini&apos;,&apos;w&apos;) as configfile: config.write(configfile) 效果 常用函数12345678910111213141516读取配置文件sections() 得到所有的section，并以列表的形式返回options(section) 得到该section的所有optionitems(section) 得到该section的所有键值对get(section,option) 得到section中option的值，返回为string类型getint(section,option) 得到section中option的值，返回为int类型getfloat(section,option)得到section中option的值，返回为float类型getboolean(section, option)得到section中option的值，返回为boolean类型写入配置文件add_section(section) 添加一个新的sectionhas_section(section) 判断是否有sectionset(section, option, value) 对section中的option进行设置remove_setion(section)删除一个sectionremove_option(section, option)删除section中的optionwrite(fileobject)将内容写入配置文件。 hashlib模块加密算法Python里面的hashlib模块提供了很多加密的算法, 比如md5(),sha1(),sha224(),sha256(),sha384(),sha512()等等 用hashlib的md5算法加密数据: python3中str在内存中是unicode形式, 被加密的内容需要时bytes类型的, 所以需要使用encode编码.12345678910111213141516import hashlib# 先生成md5对象，md5不能反解，但是加密是固定的，就是关系是一一对应，所以有缺陷，可以被对撞出来hash = hashlib.md5() # 要对哪个字符串进行加密，就放这里 hash.update(bytes(&apos;admin&apos;, encoding=&apos;utf-8&apos;)) # 返回二进制的加密结果print(hash.digest())# 十六进制的加密结果print(hash.hexdigest()) # 如果使用其他方式加密, 只需要将md5修改为对应加密算法hash = hashlib.sha384()# 如果没有参数，所有md5遵守一个规则，生成同一个对应关系，如果加了参数，就是在原先加密的基础上再加密一层，这样的话参数只有自己知道，防止被撞库，因为别人永远拿不到这个参数hash = hashlib.md5(bytes(&apos;key&apos;,encoding=&apos;utf-8&apos;)) Base64编码Base64只是一种编码算法，而非加密算法。任何人都可以将Base64的编码结果解码成唯一的原文。 Base64原理解析 什么是Base64算法 re模块处理函数 re.match函数 re.match尝试从字符串的起始位置匹配，如果起始位置匹配不成功，match()就返回none。可以使用group(num)或groups()匹配对象函数来获取匹配表达式。123456re_obj = re.match(&apos;www&apos;, &apos;www.runoob.com&apos;) # 在起始位置匹配if re_obj: print(re_obj.group(0)) # wwwre_obj = re.match(&apos;com&apos;, &apos;www.runoob.com&apos;) # 不在起始位置匹配if re_obj: print(re_obj.group(0)) # None re.search函数 re.search 扫描整个字符串并返回第一个成功的匹配。如果匹配失败返回None, 可以使用group(num)或groups()匹配对象函数来获取匹配表达式。123456re_obj = re.search(&apos;www&apos;, &apos;www.runoob.com&apos;) # 在起始位置匹配if re_obj: print(re_obj.group(0)) # wwwre_obj = re.search(&apos;com&apos;, &apos;www.runoob.com&apos;) # 不在起始位置匹配if re_obj: print(re_obj.group(0)) # com re.findall函数 在字符串中找到正则表达式所匹配的所有子串，并返回一个列表，如果没有找到匹配的，则返回空列表。 1print(re.findall(r&apos;\\d+&apos;,&apos;run88oob123google456&apos;)) # [&apos;88&apos;, &apos;123&apos;, &apos;456&apos;] 匹配规则 ^和$在MULTILINE模式下也匹配换行后的首个字符或者最后一个字符 123456789print(re.findall(r&apos;.*foo$&apos;,&apos;1foo\\n2foo\\n&apos;))print(re.findall(r&apos;.*foo$&apos;,&apos;1foo\\n2foo\\n&apos;,re.M))print(re.findall(r&apos;^foo.*&apos;,&apos;foo1\\nfoo2\\n&apos;))print(re.findall(r&apos;^foo.*&apos;,&apos;foo1\\nfoo2\\n&apos;,re.M))[&apos;2foo&apos;][&apos;1foo&apos;, &apos;2foo&apos;][&apos;foo1&apos;][&apos;foo1&apos;, &apos;foo2&apos;] ‘*’,’+’,’?’修饰符都是贪婪的, 它们在字符串进行尽可能多的匹配。 ‘*?’,’+?’,’??’是非贪婪模式匹配 12345print(re.findall(r&apos;&lt;.*&gt;&apos;,&apos;&lt;a&gt;b&lt;c&gt;&apos;))print(re.findall(r&apos;&lt;.*?&gt;&apos;,&apos;&lt;a&gt;b&lt;c&gt;&apos;))[&apos;&lt;a&gt;b&lt;c&gt;&apos;][&apos;&lt;a&gt;&apos;, &apos;&lt;c&gt;&apos;] 同理{2,4}是贪婪模式匹配, {2,4}?是非贪婪模式匹配 正则对象保存, 重复调用更高效 1234prog = re.compile(pattern)result = prog.match(string)等价于result = re.match(pattern, string) 注意即便是MULTILINE多行模式，re.match()也只匹配字符串的开始位置，而不匹配每行开始。 [] 范围, () 提取 , {} 长度12345(0-9) 匹配&apos;0-9&apos;本身[0-9]* 匹配数字,可为空[0-9]+ 匹配数字,不可为空&#123;0-9&#125; 表达式错误[0-9]&#123;0,9&#125; 长度为0-9的数字 正则表达式修饰符 官方文档 参考 在源码开头详细介绍了匹配规则 官方文档 “回车”与“换行”的区别 ‘\\r’回到行首, ‘\\n’换到下一行(0x0d 0x0a) 关于“回车”的有趣历史及“回车”与“换行”的区别 logging模块日志级别 日志等级是从上到下依次升高的，即：DEBUG &lt; INFO &lt; WARNING &lt; ERROR &lt; CRITICAL，而日志的信息量是依次减少的 logging模块提供了两种记录日志的方式： 第一种方式是使用logging提供的模块级别的函数 123456789101112131415161718192021222324252627282930import logging # 配置文件名, 文件读写模式, 日志级别, 输出格式等logging.basicConfig(level=logging.WARNING, format=&apos;%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s&apos;, datefmt=&apos;%a, %d %b %Y %H:%M:%S&apos;, filename=&apos;myapp.log&apos;, filemode=&apos;w&apos;) logging.debug(&apos;This is debug message&apos;)logging.info(&apos;This is info message&apos;)logging.warning(&apos;This is warning message&apos;)logging.basicConfig函数各参数:filename: 指定日志文件名filemode: 和file函数意义相同，指定日志文件的打开模式，&apos;w&apos;或&apos;a&apos;format: 指定输出的格式和内容，format可以输出很多有用信息，如上例所示: %(levelno)s: 打印日志级别的数值 %(levelname)s: 打印日志级别名称 %(pathname)s: 打印当前执行程序的路径，其实就是sys.argv[0] %(filename)s: 打印当前执行程序名 %(funcName)s: 打印日志的当前函数 %(lineno)d: 打印日志的当前行号 %(asctime)s: 打印日志的时间 %(thread)d: 打印线程ID %(threadName)s: 打印线程名称 %(process)d: 打印进程ID %(message)s: 打印日志信息datefmt: 指定时间格式，同time.strftime()level: 设置日志级别，默认为logging.WARNING 设置的日志级别是WARNING，因此只有WARNING级别的日志记录以及大于它的ERROR和CRITICAL级别的日志记录被输出了，而小于它的DEBUG和INFO级别的日志记录被丢弃了。 logging.basicConfig()函数是一个一次性的简单配置工具，也就是说只有在第一次调用该函数时会起作用，后续再次调用该函数时完全不会产生任何操作的，多次调用的设置并不是累加操作。 第二种方式是使用Logging日志系统的四大组件 Python之日志处理（logging模块） 示例 使用logging四大组件记录日志 1234567891011121314151617181920212223242526272829303132333435363738import loggingimport logging.handlersimport datetime# 得到一个Logger对象logger = logging.getLogger(&apos;mylogger&apos;)# 设置日志器将会处理的日志消息的最低严重级别logger.setLevel(logging.DEBUG)# 将日志消息发送到磁盘文件，并支持日志文件按时间切割rf_handler = logging.handlers.TimedRotatingFileHandler(&apos;all.log&apos;, when=&apos;midnight&apos;, interval=1, backupCount=7, atTime=datetime.time(0, 0, 0, 0))# 为handler设置一个格式器对象rf_handler.setFormatter(logging.Formatter(&quot;%(asctime)s - %(levelname)s - %(message)s&quot;))# 将日志消息发送到磁盘文件，默认情况下文件大小会无限增长f_handler = logging.FileHandler(&apos;error.log&apos;)# 设置handler将会处理的日志消息的最低严重级别f_handler.setLevel(logging.ERROR)# 为handler设置一个格式器对象f_handler.setFormatter(logging.Formatter(&quot;%(asctime)s - %(levelname)s - %(filename)s[:%(lineno)d] - %(message)s&quot;))# 将日志消息发送给一个指定的email地址(mailhost, fromaddr, toaddrs, subject, credentials=None)sh = logging.handlers.SMTPHandler((&quot;smtp.163.com&quot;, 25), &apos;****@163.com&apos;, [&apos;****@qq.com&apos;,], &quot;邮件标题&quot;, credentials=(&apos;username&apos;, &apos;password&apos;), )# 为该logger对象添加handler对象logger.addHandler(rf_handler)logger.addHandler(f_handler)logger.addHandler(sh) #将收到5条邮件(=_=)logger.debug(&apos;debug message&apos;)logger.info(&apos;info message&apos;)logger.warning(&apos;warning message&apos;)logger.error(&apos;error message&apos;)logger.critical(&apos;critical message&apos;)","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"Django-编辑器Markdown的使用","date":"2018-09-01T14:46:43.000Z","path":"2018/09/01/Django-编辑器Markdown的使用/","text":"&emsp;&emsp;之前的一篇博客写到了如何在django中使用Ueditor,今天我们看看如何在django中使用Markdown进行文本的编辑.它是基于editor.md插件 Markdown的安装Markdown使用的是源码安装的方式: 首先到github下载源码 把里面的markdown文件夹放到extra_apps中 然后在INSTALLED_APPS中添加markdown.并在url.py中添加: url(r’^markdown/‘, include(‘markdown.urls’)), 在setting.py中设置图片上传的文件夹 MARKDOWN_IMAGE_FLODER = ‘ markdown ‘ #图片会上传到media/markdown文件夹下 配置上传图片格式 MARKDOWN_IMAGE_FORMATS = [“jpg”，”jpeg”，”gif”，”png”，”bmp”，”webp”] 在admin中使用&emsp;&emsp;所有的TextField都使用markdown编辑器:12345678from django.db import modelsfrom markdown.widgets import AdminMarkdownWidgetclass TestAdmin(admin.ModelAdmin): formfield_overrides = &#123; models.TextField: &#123;&apos;widget&apos;: AdminMarkdownWidget()&#125;, &#125;admin.site.register(Test,TestAdmin) 在xadmin中使用12345678from django.db import modelsfrom markdown.widgets import XAdminMarkdownWidgetclass TestAdmin(admin.ModelAdmin): formfield_overrides = &#123; models.TextField: &#123;&apos;widget&apos;: XAdminMarkdownWidget()&#125;, &#125;xadmin.site.register(Test,TestAdmin) 在form中使用123456from django import formsfrom markdown.forms import MarkdownFieldclass BlogForm(forms.Form): name = forms.CharField() context = MarkdownField() 这里注意在前端需要添加去添加css和js1234567&lt;head&gt; &lt;title&gt;Hello Django!&lt;/title&gt; &#123;&#123;form.media&#125;&#125;&lt;/head&gt;&lt;body&gt;&#123;&#123;form&#125;&#125;&lt;/body&gt; 支持参数 widthheightthemepreviewThemeeditorThemesyncScrollingsaveHTMLToTextareaemojitaskListtocmtexflowChartsequenceDiagramcodeFold 123formfield_overrides = &#123; models.TextField: &#123;&apos;widget&apos;: AdminMarkdownWidget(emoji=False)&#125;, &#125; 最终后端效果 前端显示&emsp;&emsp;你会发现前端显示是有问题的,这个时候需要把markdown语法转换为html,我们需要安装django-markdown-deux. 首先运行 pip install django-markdown-deux 在INSTALLED_APPS中添加markdown_deux 在需要显示markdown的页面:12&#123;% load markdown_deux_tags %&#125;&#123;&#123; course.detail | markdown &#125;&#125; 最后效果: &emsp;&emsp;这里还有一个问题就是无法修改图片的大小,目前没有比较有效的解决办法,如果对图片大小要求比较高,可以使用七牛等支持参数的图床,如果有其他想法,欢迎交流~","tags":[{"name":"Django","slug":"Django","permalink":"http://yoursite.com/tags/Django/"},{"name":"文本编辑","slug":"文本编辑","permalink":"http://yoursite.com/tags/文本编辑/"}]},{"title":"Django-编辑器Ueditor的使用","date":"2018-09-01T14:46:26.000Z","path":"2018/09/01/Django-编辑器Ueditor的使用/","text":"&emsp;&emsp;Ueditor是比较流行的富文本编辑器,主要用于内容的编辑、排版和图片上传等。本文主要介绍Ueditor的安装并搭配xadmin的使用。 富文本编辑器Ueditor&emsp;&emsp;UEditor是由百度web前端研发部开发所见即所得富文本web编辑器，具有轻量，可定制，注重用户体验等特点，开源基于MIT协议，允许自由使用和修改代码。 Ueditor的源码安装&emsp;&emsp;前往github下载源码然后解压，把DjangoUeditor文件夹拷贝到项目目录(extra_apps)下面。 setting和url中的配置 settings中添加app 123INSTALLED_APPS = [ &apos;DjangoUeditor&apos;,] MxOnline/urls.py 1path(&apos;ueditor/&apos;,include(&apos;DjangoUeditor.urls&apos; )), model和adminx中的配置 course/models.py中Course修改detail字段 1234class Course(models.Model): # detail = models.TextField(&quot;课程详情&quot;) detail = UEditorField(verbose_name=u&apos;课程详情&apos;, width=600, height=300, imagePath=&quot;courses/ueditor/&quot;, filePath=&quot;courses/ueditor/&quot;, default=&apos;&apos;) xadmin/plugs目录下新建ueditor.py文件,代码如下: 1234567891011121314151617181920212223242526272829303132import xadminfrom xadmin.views import BaseAdminPlugin, CreateAdminView, ModelFormAdminView, UpdateAdminViewfrom DjangoUeditor.models import UEditorFieldfrom DjangoUeditor.widgets import UEditorWidgetfrom django.conf import settingsclass XadminUEditorWidget(UEditorWidget): def __init__(self, **kwargs): self.ueditor_options = kwargs self.Media.js = None super(XadminUEditorWidget,self).__init__(kwargs)class UeditorPlugin(BaseAdminPlugin): def get_field_style(self, attrs, db_field, style, **kwargs): if style == &apos;ueditor&apos;: if isinstance(db_field, UEditorField): widget = db_field.formfield().widget param = &#123;&#125; param.update(widget.ueditor_settings) param.update(widget.attrs) return &#123;&apos;widget&apos;:XadminUEditorWidget(**param)&#125; return attrs def block_extrahead(self, context, nodes): js = &apos;&lt;script type=&quot;text/javascript&quot; src=&quot;%s&quot;&gt;&lt;/script&gt;&apos; %(settings.STATIC_URL + &quot;ueditor/ueditor.config.js&quot;) js += &apos;&lt;script type=&quot;text/javascript&quot; src=&quot;%s&quot;&gt;&lt;/script&gt;&apos; %(settings.STATIC_URL + &quot;ueditor/ueditor.all.min.js&quot;) nodes.append(js)xadmin.site.register_plugin(UeditorPlugin, UpdateAdminView)xadmin.site.register_plugin(UeditorPlugin, CreateAdminView) xadmin/plugs/init.py里面添加ueditor插件 123PLUGINS = ( &apos;ueditor&apos;,) course/adminx.py中使用: 123class CourseAdmin(object): #detail就是要显示为富文本的字段名 style_fields = &#123;&quot;detail&quot;: &quot;ueditor&quot;&#125; xadmin后台效果&emsp;&emsp;但是前段显示还是会有问题:&emsp;&emsp;这是因为需要在模板中必须关闭Django的自动转义才能正常显示:123&#123;% autoescape off %&#125;&#123;&#123; course.detail &#125;&#125;&#123;% endautoescape %&#125; 最终显示效果","tags":[{"name":"Django","slug":"Django","permalink":"http://yoursite.com/tags/Django/"},{"name":"文本编辑","slug":"文本编辑","permalink":"http://yoursite.com/tags/文本编辑/"}]},{"title":"Python-迭代器与生成器","date":"2018-09-01T04:25:30.000Z","path":"2018/09/01/Python-迭代器与生成器/","text":"迭代器与生成器可迭代对象 如果一个对象拥有iter方法，其是可迭代对象；如果一个对象拥有next方法，其是迭代器。 定义可迭代对象，必须实现iter方法；定义迭代器，必须实现iter和next方法。所以可迭代对象包含迭代器 可以使用isinstance()判断一个对象是否是Iterable对象 可以直接作用于for循环的对象为可迭代对象。 1234567891011&gt;&gt;&gt; from collections import Iterable&gt;&gt;&gt; isinstance([], Iterable)True&gt;&gt;&gt; isinstance(&#123;&#125;, Iterable)True&gt;&gt;&gt; isinstance(&apos;abc&apos;, Iterable)True&gt;&gt;&gt; isinstance((x for x in range(10)), Iterable)True&gt;&gt;&gt; isinstance(100, Iterable)False 迭代器任何实现了iter和next方法的对象都是迭代器, 它是一个带状态的对象 iter返回迭代器自身 next返回容器中的下一个值 next()返回容器的下一个值 从头到尾访问, 不能回退 迭代到某个对象才开始计算, 节省内存 如果容器中没有更多元素了，则抛出StopIteration异常 1234&gt;&gt;&gt; a = [1,2,3,4] # list# 返回可迭代对象的迭代器实例&gt;&gt;&gt; b = iter(a) &lt;list_iterator object at 0x0000026CCE6BFEB8&gt; 生成器如果列表元素可以按照某种算法推算出来，我们可以在循环的过程中不断推算出后续的元素呢, 这样就不必创建完整的list，从而节省大量的空间，在Python中，这种一边循环一边计算的机制，称为生成器：generator 生成器是一种特殊的迭代器 生成器在迭代的过程中可以改变当前迭代值，而修改普通迭代器的当前迭代值往往会发生异常 生成器的创建 方法一 12345678910111213141516171819202122L是一个list&gt;&gt;&gt; L = [x * x for x in range(10)]&gt;&gt;&gt; L[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]g是一个生成器, 里面保存算法&gt;&gt;&gt; g = (x * x for x in range(10))&gt;&gt;&gt; g&lt;generator object &lt;genexpr&gt; at 0x1022ef630&gt;&gt;&gt;&gt; next(g)0&gt;&gt;&gt; g.__next__()1...&gt;&gt;&gt; next(g)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;StopIteration# 通过for循环来迭代它，不需要关心StopIteration错误。for n in g: print(n) 方法二 generator的另一种方法。如果一个函数定义中包含yield关键字，那么这个函数就不再是一个普通函数，而是一个generator, 在每次调用next()的时候执行，遇到yield语句返回，再次执行时从上次返回的yield语句处继续执行。12345data = fib(10) # 生成器print(data)print(data.__next__())print(data.__next__()) send实现协程操作 send可以传值, 但是是传给yeild, 需要再将yeild再赋值 send可以在传值完成后接着上次结果继续执行, 相当于next() 不能传递一个非空值给一个未启动的生成器12345678910def foo(): while True: x = yield print(&quot;value:&quot;,x)g = foo() # g是一个生成器next(g) # 程序运行到yield就停住了,等待下一个nextg.send(1) # 我们给yield发送值1,然后这个值被赋值给了x，并且打印出来,然后继续下一次循环停在yield处g.send(2) # 同上next(g) # 没有给x赋值，执行print语句，打印出None,继续循环停在yield处 解决第一次需要传递空值的办法就是装饰器123456789def deco(func): # 装饰器:用来开启协程 def wrapper(): res = func() next(res) return res # 返回一个已经执行了next()方法的函数对象 return wrapper@decodef foo(): pass 示例代码:1234567891011121314151617def deco(func): def wrapper(): res = func() next(res) return res return wrapper@decodef foo(): food_list = [] while True: food = yield food_list #返回添加food的列表 food_list.append(food) print(&quot;elements in foodlist are:&quot;,food)g = foo()print(g.send(&apos;苹果&apos;))print(g.send(&apos;香蕉&apos;))print(g.send(&apos;菠萝&apos;)) 总结 凡是可作用于for循环的对象都是Iterable类型 凡是可作用于next()函数的对象都是Iterator类型，它们表示一个惰性计算的序列； 集合数据类型如list、dict、str等是Iterable但不是Iterator，不过可以通过iter()函数获得一个Iterator对象。","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"常用Markdown语法说明","date":"2018-08-30T13:10:19.000Z","path":"2018/08/30/Markdown-常用语法说明/","text":"Markdown 是一种轻量级标记语言，它允许人们“使用易读易写的纯文本格式”编写文档，本文主要针对Hexo中Markdown常用的语法进行总结，更详细的内容请参考Hexo官方书写格式链接和Markdown书写格式链接 标题Markdown支持两种标题的语法，Setext和atx形式：Setext形式是用底线的形式，利用 = (最高阶标题)和 - (第二阶标题)。Atx形式在行首插入 1 到 6 个 # ，对应到标题 1 到 6 阶。 最高阶标题第二阶标题H1标题H2标题H3标题H4标题H5标题H6标题区块引用区块引用则使用 email 形式的 ‘&gt;’ 角括号。 区块引用 嵌套引用 修辞和强调强调内容两侧分别加上星号或者底线。斜体粗体删除 表格 至少一个- 使用:来设置对其方式 第一列 第二列 第三列 内容 内容 内容 列表无序列表使用星号、加号或减号来做为列表。 一号 二号 三号 四号 五号 六号 有序列表 一号 二号 链接这是我的博客地址 分割线 图片图片的语法和链接很像，只需在链接的基础上前方加一个！注意图片大小的设置方式,关于其他设置可以参考七牛云 代码main.c12345int i = 0; i = 1; for (int i = 0; i &lt; 100; i++)&#123; printf(\"hello markdown!\\n\");&#125; 任务列表 选项一 选项二 选项三","tags":[{"name":"Markdown","slug":"Markdown","permalink":"http://yoursite.com/tags/Markdown/"}]},{"title":"Python-面向对象编程","date":"2018-08-27T12:25:30.000Z","path":"2018/08/27/Python-面向对象编程/","text":"面向对象编程变量和方法 类的变量存在类的内存中, 实例变量存在实例内存中 对于变量,先找实例本身是否有,如果找不到就去类中找 静态属性就是变量, 动态属性就是方法 实例变量作用域是实例本身 封装类的实例化是以类为模板在内存中开辟空间存数据, 赋予变量名 继承继承是一种创建新类的方式，在python中，新建的类可以继承一个或多个父类，父类又可称为基类或超类，新建的类称为派生类或子类 多继承的查找方式 super 类的继承: 子类中重新定义某个方法会覆盖父类的同名方法, 如果要同时实现父类的功能, 就要使用super super是用来解决多重继承问题的，直接用类名调用父类方法在使用单继承的时候没问题，但是如果使用多继承，会涉及到查找顺序（MRO）、重复调用等种种问题。 Python: 你不知道的 super Python’s super() considered super! 下划线 123456789101112131415161718192021222324252627282930313233# 类中所有双下划线开头的名称如__x都会在类定义时自动变形成：_类名__x的形式class A(): __N = 0 _M = 1 # 可以在外部调用, 只是给程序员提示 def __init__(self): self.__N = 10 # _A__N def read(self): print(self.__N) def __write(self): # _A__write self.__N = 20 print(self.__N) def write(self): self.__write() # 可以在内部调用print(dir(A))a = A()print(a._M)a.read()# a.__write() # 无法调用a.write()print(a._A__N) # 可以调用a._A__write() # 可以调用[&apos;_A__N&apos;, &apos;_A__write&apos;, &apos;_M&apos;, &apos;__class__&apos;, &apos;__delattr__&apos;, &apos;__dict__&apos;, &apos;__dir__&apos;, &apos;__doc__&apos;, &apos;__eq__&apos;, &apos;__format__&apos;, &apos;__ge__&apos;, &apos;__getattribute__&apos;, &apos;__gt__&apos;, &apos;__hash__&apos;, &apos;__init__&apos;, &apos;__init_subclass__&apos;, &apos;__le__&apos;, &apos;__lt__&apos;, &apos;__module__&apos;, &apos;__ne__&apos;, &apos;__new__&apos;, &apos;__reduce__&apos;, &apos;__reduce_ex__&apos;, &apos;__repr__&apos;, &apos;__setattr__&apos;, &apos;__sizeof__&apos;, &apos;__str__&apos;, &apos;__subclasshook__&apos;, &apos;__weakref__&apos;, &apos;read&apos;, &apos;write&apos;]110202020 Python中下划线的5种含义 抽象类与接口类 抽象类: 只能被继承, 不能实例化, 只能有抽象方法, 没有实现功能, 子类必须实现抽象方法(规范化) 接口继承实现归一化 123456789101112131415python3:from abc import ABCMeta, abstractmethodclass Pay(metaclass=ABCMeta): # 规范一个方法上面加一句@abstractmethod @abstractmethod def pay(self): pass python2:class Pay(object): __metaclass__ = ABCMeta # 规范一个方法上面加一句@abstractmethod @abstractmethod def pay(self): pass Python_015(面向对象(接口类,抽象类,多态,封装) 多态封装可以隐藏实现细节，使得代码模块化；继承可以扩展已存在的代码模块（类）；它们的目的都是为了——代码重用。而多态则是为了实现另一个目的——接口重用！多态的作用，就是为了类在继承和派生的时候，保证使用“家谱”中任一类的实例的某一属性时的正确调用。1234567891011121314151617181920212223242526272829303132333435class Animal(object): def __init__(self, name): # Constructor of the class self.name = name def talk(self): raise NotImplementedError(&quot;Subclass must implement abstract method&quot;)class Cat(Animal): def talk(self): print(&apos;%s: 喵喵喵!&apos; % self.name)class Dog(Animal): def talk(self): print(&apos;%s: 汪！汪！汪！&apos; % self.name)def func(obj): # 一个接口，多种形态 obj.talk()c1 = Cat(&apos;小晴&apos;)d1 = Dog(&apos;李磊&apos;)func(c1)func(d1)# 多态性带来的好处，比如Python的序列类型有多种形态：字符串，列表，元组，多态性体现# str,list,tuple都是序列类型s=str(&apos;hello&apos;)l=list([1,2,3])t=tuple((4,5,6))# 我们可以在不考虑三者类型的前提下使用s,l,ts.__len__()l.__len__()t.__len__() 三种方法实例方法 可以访问实例变量和类变量 只能实例对象调用 静态方法 @staticmethod 静态方法是不可以访问实例变量或类变量 可通过实例对象或类对象调用 不会主动传入self, 调用时可以传入实例本身或者去掉self 类方法 @classmethod 类方法只能访问类变量，不能访问实例变量 可通过实例对象或类对象调用 需要传入cls参数, cls参数指向的是一开始定义的类对象（不是实例对象） Django中classonlymethod继承了classmethod，但是classonlymethod只能通过类对象调用，而不能通过实例对象调用as_view()方法，即‘阉割’了实例调用的方式。 类属性 @property 调用不需要添加() 传参123456789@propertydef password(self): return self._password@password.setterdef password(self, raw): self._password = generate_password_hash(raw)self.password = &apos;123456&apos; 对比和适用场景123456789101112131415161718192021222324252627282930313233class MyClass(object): # 实例方法 def instance_method(self): print(&apos;instance method called&apos;, self) # 类方法 @classmethod def class_method(cls): print(&apos;class method called&apos;, cls) # 静态方法 @staticmethod def static_method(): print(&apos;static method called&apos;)# 通过实例对象调用my_class = MyClass() # 实例化my_class.static_method() # 静态方法my_class.class_method() # 类方法my_class.instance_method() # 实例方法# 通过类对象调用MyClass.static_method() # 静态方法MyClass.class_method() # 类方法MyClass.instance_method() # 实例方法static method calledclass method called &lt;class &apos;__main__.MyClass&apos;&gt;instance method called &lt;__main__.MyClass object at 0x0000021CE87A1160&gt;static method calledclass method called &lt;class &apos;__main__.MyClass&apos;&gt;# 报错 instance_method() missing 1 required positional argument: &apos;self&apos; 类方法无须创建实例对象调用，所以类方法的调用较实例方法更为灵活 静态方法有点像附属于类对象的“工具”, 将对象的相关处理逻辑“束缚”在对象体内，这样封装得会更好些。 实例方法只能通过实例对象调用；类方法和静态方法可以通过类对象或者实例对象调用，如果是使用实例对象调用的类方法或静态方法，最终都会转而通过类对象调用。 实例方法使用最多，可以直接处理实例对象的逻辑；类方法不需要创建实例对象，直接处理类对象的逻辑；静态方法将与类对象相关的某些逻辑抽离出来，不仅可以用于测试，还能便于代码后期维护。 实例方法和类方法，能够改变实例对象或类对象的状态，而静态方法不能。 类的特殊方法 __doc__ 类的描述信息(注释) __str__ 打印对象时，默认输出该方法的返回值。 __init__ 创建对象后，python解释器默认调用该方法 __del__ 当删除一个对象时，python解释器也会默认调用该方法 __call__ 对象后加()时执行, 对象 = 类名() __module__ 表示当前操作的对象在哪个模块 __class__ 表示当前操作的对象的类是哪个 __dict__ 查看类或对象中的所有成员 1234567891011class MyClass(object): def __init__(self): print(&apos;init&apos;) def __call__(self, *args, **kwargs): print(&apos;call&apos;)# initmyclass = MyClass()# callmyclass() 反射执行某个方法, 调用某个变量,要确定该方法或者属性是否存在 hasattr：否有存在对应属性或者方法 getattr: 获取属性或者方法 setattr: 添加属性或者方法 delattr：删除属性1234567891011121314151617181920212223class Foo(object): def __init__(self): self.name = &apos;rearib&apos; def func(self): print(&apos;func&apos;)obj = Foo() # 检查是否含有成员hasattr(obj, &apos;name&apos;) # Turehasattr(obj, &apos;func&apos;) # Ture # 获取成员getattr(obj, &apos;name&apos;) # rearibgetattr(obj, &apos;func&apos;) # &lt;bound method Foo.func of &lt;__main__.Foo object at 0x00000268D131EA58&gt;&gt; # 设置成员setattr(obj, &apos;age&apos;, 18)setattr(obj, &apos;show&apos;, lambda num: num + 1) # #### 删除成员 ####delattr(obj, &apos;name&apos;)delattr(obj, &apos;name&apos;) # 如果属性不存在, 报错delattr(obj, &apos;func&apos;) # 不能删除对象方法, 报错","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"Python-数据类型","date":"2018-08-26T11:25:30.000Z","path":"2018/08/26/Python-数据类型/","text":"数据类型字符串字符串内置函数 列表列表排序reverse、sort、sorted reverse() 反转, 不会排序 sort() 排序, 排序后的新列表会覆盖原列表(默认正向排序) sorted() 排序, 可以保留原列表, 又能得到已经排序好的列表(默认正向排序) a[::-1] 反转, 不会排序 12345678910111213141516&gt;&gt;&gt; x = [1,5,2,3,4]&gt;&gt;&gt; x.reverse()&gt;&gt;&gt; x[4, 3, 2, 5, 1]&gt;&gt;&gt; a = [5,7,6,3,4,1,2]&gt;&gt;&gt; a.sort()&gt;&gt;&gt; a[1, 2, 3, 4, 5, 6, 7]&gt;&gt;&gt; a = [5,7,6,3,4,1,2]&gt;&gt;&gt; b = sorted(a)&gt;&gt;&gt; a[5, 7, 6, 3, 4, 1, 2]&gt;&gt;&gt; b[1, 2, 3, 4, 5, 6, 7] enumerate同时需要用到index和value值的时候可以用到enumerate，参数为可遍历的变量，如字符串，列表等，返回enumerate类。 (0, seq[0]), (1, seq[1]), (2, seq[2]), ... 元组 index, count 字典 items(), keys(), values() dict会把所有的key变成hash表，然后将这个表进行排序，这样，你通过data[key]去查data字典中一个key的时候，python会先把这个key hash成一个数字，然后拿这个数字到hash表中看没有这个数字，如果有，拿到这个key在hash表中的索引，拿到这个索引去与此key对应的value的内存地址那取值就可以了。 集合 set() add() 添加一个 update() 添加多个 remove() 移除 交集, 并集, 差集 可以使用大括号{}或者set()函数创建集合, 注意: 创建一个空集合必须用set()而不是{}, 因为{}是用来创建一个空字典。123parame = &#123;value01,value02,...&#125; # 重复的元素被自动去掉orset(value) 总结 数字 字符串 列表 [] 元组 () 字典 {} 集合 set() 深浅拷贝变量-引用-对象在Python中一切都是对象, Python中变量是指对象的引用，Python是动态类型，程序运行时候，会根据对象的类型来确认变量到底是什么类型。 运行a=3后，变量a变成了对象3的一个引用。在内部，变量事实上是到对象内存空间的一个指针 Python的变量不过是对象的引用，或指向对象的指针，因此在程序中可以经常改变变量引用 123&gt;&gt;&gt; x = 42 #变量绑定到整型对象&gt;&gt;&gt; x = &apos;Hello&apos; #现在又成了字符串&gt;&gt;&gt; x = [1,2,3] #现在又成了列表 可变与不可变对象 在Python中不可变对象指：一旦创建就不可修改的对象，包括字符串，元祖，数字(a=b, a的值发生改变，b不会跟着改变) 在Python中可变对象是指：可以修改的对象，包括：列表、字典(a=b, a的值发生改变，b也跟着改变) 深拷贝与浅拷贝(针对可变对象) 浅拷贝：只拷贝顶级的对象，或者说：父级对象 深拷贝：拷贝所有对象，顶级对象及其嵌套对象。或者说：父级对象及其子对象 深浅拷贝都是对源对象的复制，占用不同的内存空间 如果源对象只有一级目录的话，源做任何改动，不影响深浅拷贝对象 如果源对象不止一级目录的话，源做任何改动，都要影响浅拷贝，但不影响深拷贝 序列对象的切片其实是浅拷贝，即只拷贝顶级的对象 要想完全无关, 需要使用深拷贝","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"Python-话说编码","date":"2018-08-16T11:25:10.000Z","path":"2018/08/16/Python-话说编码/","text":"编码 使用编辑器编写py文件保存只是将内存中unicode编码的内容使用系统或者编辑器的编码方式保存到硬盘中. python解释器执行py文件首先是作为一个编辑器将py文件从硬盘中读取到内存, 然后再进行解释. python2默认ascii, python3默认utf-8, 在python2文件中有#coding:utf-8, 那么文件会以utf-8方式解码成unicode放到内存中.最终到内存中都是unicode. 但是程序还没有开始解释. 读取已经加载到内存的代码（unicode编码的二进制），然后执行，但是程序在执行过程中，会申请内存（与程序代码所存在的内存是俩个空间），可以存放任意编码格式的数据，比如x=”egon”,会被python解释器识别为字符串，会申请内存空间来存放”egon”，然后让x指向该内存地址，此时新申请的该内存地址保存也是unicode编码的egon,如果代码换成x=”egon”.encode(‘utf-8’),那么新申请的内存空间里存放的就是utf-8编码的字符串egon了 python3中两种字符串类型str和bytesstr是unicode bytes是bytes 1234567891011121314#coding:utf-8s=&apos;林&apos; #当程序执行时，无需加u，&apos;林&apos;也会被以unicode形式保存新的内存空间中,#s可以直接encode成任意编码格式s1=s.encode(&apos;utf-8&apos;)s2=s.encode(&apos;gbk&apos;)print(s) #林print(s1) #b&apos;\\xe6\\x9e\\x97&apos; 在python3中，是什么就打印什么print(s2) #b&apos;\\xc1\\xd6&apos; 同上print(type(s)) #&lt;class &apos;str&apos;&gt;print(type(s1)) #&lt;class &apos;bytes&apos;&gt;print(type(s2)) #&lt;class &apos;bytes&apos;&gt; python2中两种字符串类型str和unicode在python2中，str就是编码后的结果bytes，str=bytes,所以在python2中，unicode字符编码的结果是str/bytes 当python解释器执行到产生字符串的代码时（例如s=’林’），会申请新的内存地址，然后将’林’encode成文件开头指定的编码格式 12345678#coding:utf-8s=&apos;林&apos; #在执行时,&apos;林&apos;会被以conding:utf-8的形式保存到新的内存空间中print repr(s) #&apos;\\xe6\\x9e\\x97&apos; 三个Bytes,证明确实是utf-8print type(s) #&lt;type &apos;str&apos;&gt;s.decode(&apos;utf-8&apos;)# s.encode(&apos;utf-8&apos;) #报错，s为编码后的结果bytes，所以只能decode 当python解释器执行到产生字符串的代码时（例如s=u’林’），会申请新的内存地址，然后将’林’以unicode的格式存放到新的内存空间中，所以s只能encode，不能decode 1234567s=u&apos;林&apos; print repr(s) #u&apos;\\u6797&apos;print type(s) #&lt;type &apos;unicode&apos;&gt;# s.decode(&apos;utf-8&apos;) #报错，s为unicode，所以只能encodes.encode(&apos;utf-8&apos;) 打印到终端print(x) #这一步是将x指向的那块新的内存空间（非代码所在的内存空间）中的内存，打印到终端，而终端仍然是运行于内存中的，所以这打印可以理解为从内存打印到内存，即内存-&gt;内存，unicode-&gt;unicode 对于unicode格式的数据来说，即python3中的字符串与python2中的u’字符串’，都是unicode，所以无论如何打印都不会乱码 在python2中存在另外一种非unicode的字符串，此时，print x，会按照终端的编码执行x.decode(‘终端编码’)，变成unicode后，再打印，此时终端编码若与文件开头指定的编码不一致，乱码就产生了(在windows终端, 终端编码为gbk, x指向的内容是utf-8编码的, 就会出现乱码) b和bytes12345678910111213print(&quot;Hello&quot;)print(type(&quot;Hello&quot;))print(bytes(&apos;Hello&apos;,encoding=&apos;utf-8&apos;))print(b&quot;Hello&quot;)print(type(b&quot;Hello&quot;))Hello&lt;class &apos;str&apos;&gt;b&apos;Hello&apos;b&apos;Hello&apos;&lt;class &apos;bytes&apos;&gt; 常见编码错误的原因： Python解释器的默认编码 Python源文件文件编码 Terminal使用的编码 操作系统的语言设置 参考 python编码问题大终结 Python（字符编码）","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"linux-Linux基础知识","date":"2018-08-11T04:03:44.000Z","path":"2018/08/11/linux-Linux基础知识/","text":"&emsp;&emsp;作为一个程序猿，基本都会接触 linux , 所以一些基本的操作和指令还是要掌握的， 这里整理出平时使用 linux 常用的一些指令。linux命令大全：传送门 Linux系统目录结构 目录 解释 备注 bin 存放常用可执行文件或者链接文件 (绿色或浅蓝色)ls,cat,mkdir等常用命令一般都在这里 etc 存放系统管理所需要的配置文件和子目录 home 用户的主目录 在Linux中，每个用户都有一个自己的目录,用户创建时会在该目录下创建一个目录,目录名是以用户的账号命名 root 该目录为系统管理员，也称作超级权限者的用户主目录 root的家目录 tmp 存放一些临时文件 boot Linux时使用的一些核心文件 存储系统启动文件 dev Linux的硬件设备目录 linux系统所有的硬件都是通过文件表示 lib 系统最基本的动态连接共享库 media linux系统会自动识别一些设备 例如U盘、光驱等等，当识别后，linux会把识别的设备挂载到这个目录下 mnt 用户临时挂载别的文件系统,，我们可以将光驱挂载在/mnt/上，然后进入该目录就可以查看光驱里的内容 sbin s就是Super User的意思，这里存放的是系统管理员使用的系统管理程序 该目录文件对应指令都是root用户可以执行的 usr 用户的很多应用程序和文件都放在这个目录下,类似于windows下的program files目录 该目录经常用于安装各种软件 var 这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件 /var：这是一个非常重要的目录，系统上跑了很多程序，那么每个程序都会有相应的日志产生，而这些日志就被记录到这个目录下，具体在/var/log 目录下 /bin,/sbin,/usr/sbin,/usr/bin目录 使用权限 /bin下的命令管理员和一般的用户都可以使用 /sbin下的命令通常只有管理员才可以运行。 命令功能 /sbin 下的命令属于基本的系统命令，如shutdown，reboot，用于启动系统，修复系统 /bin下存放一些普通的基本命令，如ls,chmod等，这些命令在Linux系统里的配置文件脚本里经常用到。 /usr/sbin,/usr/bin存放一些系统非必须的命令 /usr/bin 是你在后期安装的一些软件的运行脚本。主要放置一些应用软体工具的必备执行档 /usr/sbin 放置一些用户安装的系统管理的必备程式 参考文章：传送门 &emsp;&emsp;如果新装的系统，运行一些很正常的诸如：shutdown，fdisk的命令时，悍然提示：bash:command not found。那么首先就要考虑root 的$PATH里是否已经包含了这些环境变量。 目录与文件 功能 命令 备注 白色 普通文件 蓝色 文件夹 [d] 浅蓝色 链接文件 [l] 红色 压缩文件 [-] 红色闪烁 链接文件有问题 绿色 可执行文件 黄色 设备文件 灰色 其他文件 进入一个目录 cd xxx 列出当前目录文件列表 ls -al -l：长格式显示-a：全部的文件 返回上一目录 cd .. 返回根目录 cd / 显示当前目录路径 pwd 返回上一次所在目录 cd - 创建目录 mkdir xx 创建文件 touch xx 复制 cp [选项] 原文件 目标路径 -r：递归，复制目录时必须有此选项-p：保持原文件的权限、修改时间等属性不变 移动 mv [选项] 原文件 目标路径 删除 rm [选项] 文件或目录 -r：递归删除（含目录）-f：强制删除 删除空的目录 rmdir xxx 由第一行开始显示文件内容 cat xxx 显示最后一屏内容 从最后一行开始显示文件内容 tac xxx 显示的时候，同时输出行号 nl xxx 一页一页的显示文件内容(不支持回看) more Space：代表向下翻一页b：往回翻页Enter：向下翻一行/xx：向下搜索xxq：立刻离开 more 一页一页的显示文件内容(支持回看) less Space或PgDn：代表向下翻一页PgUp：往回翻页/xx：向下搜索xx?xx：向前搜索xxq：立刻离开 more 取出文件前面几行 head -n 文件名 -n ：后面接数字，代表显示几行 取出文件后面几行 tail -n 文件名 -n ：后面接数字，代表显示几行 读写执行 rwx r:读[4]w:写[2]x:执行[1]-:没有权限[0] 更改文件所属用户组 chgrp [-R] 属组名 文件名 更改文件所属用户和所属用户组 chown [–R] 属主名 文件名chown [-R] 属主名：属组名 文件名 更改文件9个属性 chmod [-R] 777 文件名 -R : 进行递归(recursive)的持续变更 目录对权限的使用 读：是否可以查看该目录内部的文件信息 写：是否可以给该目录创建、删除文件 执行：指定用户是否可以cd进入该目录 用户和用户组管理 功能 命令 备注 添加新的用户账号 useradd [选项] 用户名 会在home文件夹下创建一个名字为用户名的文件夹 删除用户账号 userdel [选项] 用户名 修改用户账号 usermod [选项] 用户名 修改用户密码 passwd [选项] 用户名 用户账号刚创建时没有口令，但是被系统锁定，必须设置口令后才能使用 增加一个新的用户组 groupadd [选项] 用户组 删除用户组 groupdel 用户组 修改用户组的属性 groupmod [选项] 用户组 &emsp;&emsp;不同的用户在不同的组,所以对不同的文件有不同的操作权限,root只需要关心用户属于哪个组/etc/passwd存储的用户信息: 磁盘管理 功能 命令 备注 查看文件系统的整体磁盘使用量 df [选项] 目录或文件名 -h ：以人们较易阅读的 GBytes, MBytes, KBytes 等格式自行显示 查看文件和目录磁盘使用的空间 du [选项] 目录或文件名 磁盘分区操作 fdisk fdisk -l，系统将会把整个系统内能够搜寻到的装置的分区均列出来 vim命令 功能 命令 备注 移动到这个档案的第一行 gg 移动到这个档案的最后一行 G 光标移到该行最前面 0或[home] 光标移到该行最后面 $或[End] 屏幕向下移动一页 [Ctrl] + [f]或[Page Down] 屏幕向上移动一页 [Ctrl] + [b]或[Page Up] 向光标之下寻找一个名称为 word 的字符串 /word 向光标之上寻找一个字符串名称为 word 的字符串 ?word 重复前一个搜寻的动作 n 反向进行前一个搜寻动作 N 删除游标所在的那一整行 dd 复制游标所在的那一整行 yy 使用word2替换word1 :%s/word1/word2/g /gc表示需要确认 后退(撤销) u 前进(重新执行) [Ctrl]+r 重复前一个动作 . 进入输入模式 i 将编辑的数据写入硬盘档案中 w w!为强制写入 离开 q q!为强制离开 储存后离开 :wq wq! 则为强制储存后离开 压缩文件 功能 命令 备注 打包并压缩文件 tar -zcvf test.tar.gz ./test/ 打包后，以 gzip 压缩 解压 tar -zxvf test.tar.gz 其他常用操作查看各个命令的使用文档 man cp 查找 find /home -name “*.txt” 在/home目录下查找以.txt结尾的文件名，如果没有指定目录会在当前目录和子目录下查找（硬盘）which ls 定位/返回ls所在的路径(查找可执行文件) 软件操作命令 软件管理包：yum 安装软件：yum install xxx 卸载软件：yum remove xxx 搜索软件：yum search xxx 清理缓存：yum clean packages 列出已按照：yum list 软件包信息：yum info xxx (yum info vim-commo) 服务器硬件资源和磁盘操作 内存：free -m 硬盘：df -h 负载：w/top cpu个数和核数：cat /proc/cpuinfo log查看 日志以文本可以存储在“/var/log/”目录下后缀名为.log 查看所有进程 ps aux | ps -ef 查看所有进程ps -ef | grep java 查看所有进程里 CMD 是 java 的进程信息 ps aux 是用BSD的格式来显示 java这个进程 显示的项目有：USER , PID , %CPU , %MEM , VSZ , RSS , TTY , STAT , START , TIME , COMMANDps -ef 是用标准的格式显示java这个进程 显示的项目有：UID , PID , PPID , C , STIME , TTY , TIME , CMD UID //用户ID、但输出的是用户名PID //进程的IDPPID //父进程IDC //进程占用CPU的百分比STIME //进程启动到现在的时间TTY //该进程在那个终端上运行，若与终端无关，则显示? 若为 pts/0等，则表示由网络连接主机进程。CMD //命令的名称和参数 kill 命令用于终止进程 kill -9 [PID] -9 表示强迫进程立即停止 通常用 ps 查看进程 PID ，用 kill 命令终止进程 Linux 查看某个服务的端口 netstat -anp | grep ssh 查看以开放哪些端口 netstat -nupl (UDP类型的端口)netstat -ntpl (TCP类型的端口) 端口不是独立存在的，它是依附于进程的。某个进程开启，那么它对应的端口就开启了，进程关闭，则该端口也就关闭了。下次若某个进程再次开启，则相应的端口也再次开启。而不要纯粹的理解为关闭掉某个端口，不过可以禁用某个端口。 关掉对应的应用程序，则端口就自然关闭了 linux 中 find 和 grep 的区别 Linux 系统中 grep 命令是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。 find 命令从指定的起始目录开始，递归地搜索其各个子目录，查找满足寻找条件的文件并对之采取相关的操作。 grep 是查找匹配条件的行，find 是搜索匹配条件的文件 管道pipe变量修饰器/管道：前者的输出是后者的输入参数。 ls -l | wc //计算当前目录一共有多少个文件 ls -l | head -10 //以详细列表形式查看当前目录下前10个文件 查看ip ifconfig ip addr 替换默认源 1、备份系统源cd /etc/yum.repos.dmv CentOS-Base.repo CentOS-Base.repo.bak 2、设置默认源为163(注意linux版本为7.3) wget https://mirrors.163.com/.help/CentOS7-Base-163.repomv CentOS7-Base-163.repo CentOS-Base.repo 3、执行yum源更新 yum clean all // 清除以前的缓存yum makecache // 重建缓存yum update //源的服务器更新了之后旧的包可能被新的替代了，所以需要yum update或者yum upgrade 切换用户 su admin 输入密码传送门 Linux链接概念&emsp;&emsp;Linux 链接分两种，一种被称为硬链接（Hard Link），另一种被称为符号链接（Symbolic Link）。默认情况下，ln 命令产生硬链接。 硬连接&emsp;&emsp;硬连接指通过索引节点来进行连接。在 Linux 的文件系统中，保存在磁盘分区中的文件不管是什么类型都给它分配一个编号，称为索引节点号(Inode Index)。在 Linux 中，多个文件名指向同一索引节点是存在的。比如：A 是 B 的硬链接（A 和 B 都是文件名），则 A 的目录项中的 inode 节点号与 B 的目录项中的 inode 节点号相同，即一个 inode 节点对应两个不同的文件名，两个文件名指向同一个文件，A 和 B 对文件系统来说是完全平等的。删除其中任何一个都不会影响另外一个的访问。&emsp;&emsp;硬连接的作用是允许一个文件拥有多个有效路径名，这样用户就可以建立硬连接到重要文件，以防止“误删”的功能。其原因如上所述，因为对应该目录的索引节点有一个以上的连接。只删除一个连接并不影响索引节点本身和其它的连接，只有当最后一个连接被删除后，文件的数据块及目录的连接才会被释放。也就是说，文件真正删除的条件是与之相关的所有硬连接文件均被删除。 软连接&emsp;&emsp;另外一种连接称之为符号连接（Symbolic Link），也叫软连接。软链接文件有类似于 Windows 的快捷方式。它实际上是一个特殊的文件。在符号连接中，文件实际上是一个文本文件，其中包含的有另一文件的位置信息。比如：A 是 B 的软链接（A 和 B 都是文件名），A 的目录项中的 inode 节点号与 B 的目录项中的 inode 节点号不相同，A 和 B 指向的是两个不同的 inode，继而指向两块不同的数据块。但是 A 的数据块中存放的只是 B 的路径名（可以根据这个找到 B 的目录项）。A 和 B 之间是“主从”关系，如果 B 被删除了，A 仍然存在（因为两个是不同的文件），但指向的是一个无效的链接。&emsp;&emsp;硬连接文件 f2 与原文件 f1 的 inode 节点相同，均为 9797648，然而符号连接文件的 inode 节点不同。&emsp;&emsp;当删除原始文件 f1 后，硬连接 f2 不受影响，但是符号连接 f1 文件无效 总结 1.删除符号连接f3,对f1,f2无影响； 2.删除硬连接f2，对f1,f3也无影响； 3.删除原文件f1，对硬连接f2没有影响，导致符号连接f3失效； 4.同时删除原文件f1,硬连接f2，整个文件会真正的被删除。 软链接link 就是win的快捷方式: ln -s 源文件 软链接 源文件 被删除，对应的软链接就变为”无效链接”，如果再创建一个同名源文件，软链接又恢复为有效链接文件。硬链接 ln [-d] 源文件 硬链接 系统里边文件的名称(引用)就是硬链接,一个文件可以有多个名字，它们都是同一个文件实体的硬链接 同一个文件实体可以创建多个文件名字，他们都是”硬链接”，好处是即使删除其中的一个名字，也可以保证文件实体被垃圾回收机制给收走。 只有普通文件可以设置硬链接，目录不可以 同一个源文件的所有硬链接文件必须 在同一个硬盘、同一个分区里边 当 rm 一个文件的时候，那么此文件的硬链接数减1，当硬链接数为 0 的时候，文件被删除。 操作系统分区原理win系统分区原理 linux系统分区原理 ssh原理https://www.jianshu.com/p/33461b619d53","tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"Python-文件","date":"2018-08-06T12:25:30.000Z","path":"2018/08/06/Python-文件/","text":"文件常用方法 read() 读取文件所有内容 readline() 只读取一行内容 readlines() 把文章内容以换行符分割，并生成list格式（数据量大的时候不建议用） tell() 获取文件读取指针的位置 seek() 移动文件读写指针到指定的位置 flush() 手动刷新缓冲区，将缓冲区中的数据立刻写入文件，同时清空缓冲区 with语句 二进制操作 默认光标在起始的位置，read()读取完后，光标停留到文件末尾 访问模式 访问模式 说明 指针位置 文件存在 文件不存在 r 只读 开头 从头读取内容 报错 w 只写 开头 擦除写入新内容 创建新文件写入新内容 a 追加 结尾 结尾追加新内容 创建新文件写入新内容 rb 二进制格式只读 开头 从头读取内容 报错 wb 二进制格式只写 开头 擦除写入新内容 创建新文件写入新内容 ab 二进制格式追加 结尾 结尾追加新内容 创建新文件写入新内容 r+ 读写方式打开 开头 从头读或覆盖写 报错 w+ 读写方式打开 开头 清空之前的内容 创建新文件写入新内容 a+ 读写方式打开 结尾 结尾追加新内容, 读取返回空 创建新文件写入新内容 rb+ 二进制格式读写 开头 从头读或覆盖写 报错 wb+ 二进制格式读写 开头 清空之前的内容 创建新文件写入新内容 ab+ 二进制格式读写 结尾 结尾追加新内容, 读取返回空 创建新文件写入新内容 1234if os.path.exists(File): outFile=open(File, &apos;r+&apos;) # 如果文件存在追加else: outFile=open(File, &apos;w+&apos;) # 如果不存在创建","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"工具-ShareX+七牛云实现私有图床","date":"2018-06-21T06:43:22.000Z","path":"2018/06/21/Tool-ShareX+七牛云实现私有图床/","text":"&emsp;&emsp;所谓图床，就是专门用来存放图片，同时允许你把图片对外连接的网上空间，通过访问链接就能直接查看或者分享图片，特别是个人博客的图片存储需要一个方便高效的私有图床，今天介绍一个通过ShareX+七牛云实现私有图床的方法。 什么是七牛云&emsp;&emsp;七牛云是国内一家领先的云存储公司，可以利用七牛云存储对象存储图片并生成外链。官网地址，注册后会赠送10G的免费空间. 什么是ShareX&emsp;&emsp;截图，这种再寻常不过的事情。有着很多的软件供我们选择，从常用的QQ到使用Snipaste。不光截图的方式多种多样，从处理截图，到上传截图到网络，都有软件能为你服务。但是，从截图，到处理截图，再到上传截图，至少需要用到两个软件。但是一个ShareX就能全部搞定。 用ShareX+qiniu一键截图上传配置1. sharex下载&emsp;&emsp;安装后打开如图 2. 设置图片上传&emsp;&emsp;打开 shareX -&gt; 目的地 -&gt; 目的地设置 -&gt; 自定义上传（最底下） 上传者：填入 qiniu，点击添加 请求 URL：进入自己的「对象存储」-&gt; 空间设置 -&gt; 存储区域，查看自己所在地区。根据 存储区域 - 七牛开发者中心 选择自己的地址，我是华东，填入 up.qiniup.com URL（右侧靠下）:自己的外链默认域名 + $json:key$ -&gt; xxxxx.bkt.clouddn.com/$json:key$ 上图中我的默认域名修改成了自己的默认域名qiniu.rearib.top 3. 重点：生成 token下载生成工具下载生成工具后执行： qiniutoken.exe -ak=[AccessKey] -sk=[SecretKey] -bk=[bucket] AccessKey和SecretKey登录七牛云-个人面板-秘钥管理-AK|SK bucket是存储空间名字 参数 &emsp;&emsp;在「参数」下有两个输入空框 左框 右框 token 上部生成的 token key %yy%d%h%mi%s.png file $input$ key 为上传时用的文件名，%yy%d%h%mi%s.png 是指年月日小时分钟秒，如 20180213165812.png 4.测试&emsp;&emsp;都配置完后，点击左侧「图片上传」-「测试」，若测试结果返回一个正常的连接地址那就是配置成功了。&emsp;&emsp;设置上传地址：sharex -&gt; 目的地 -&gt; 图片上传 -&gt; 自定义图像上传 &emsp;&emsp;截图后的动作 -&gt; 上传图片 按下快捷键截图后会自动上传并把七牛云返回的外链放到剪切板，成功后会有提示音并显示状态.打开浏览器粘贴 参考链接1、用 ShareX qiniu 一键截图上传 2、ShareX使用七牛文件上传 3、ShaseX","tags":[{"name":"Tool","slug":"Tool","permalink":"http://yoursite.com/tags/Tool/"}]}]